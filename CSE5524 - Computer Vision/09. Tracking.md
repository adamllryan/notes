---
id: 09. Tracking
aliases: []
tags: []
---

# Feature-based Tracking

## Lucas-Kanade Optical Flow ALgorithm

We solve for optical flow vector d, where $d=G^{-1}b$. Here, G is the gradient matrix and b is the error vector.

The gradient matrix G is defined as:

$$
G = \begin{bmatrix}
\sum I_x^2 & \sum I_xI_y \\
\sum I_xI_y & \sum I_y^2
\end{bmatrix}
=
\sum\sum
\begin{bmatrix}
I_x^2 & I_xI_y \\
I_xI_y & I_y^2
\end{bmatrix}
$$

The error vector b is defined as:

$$
b = \begin{bmatrix}
-\sum I_xI_t \\
-\sum I_yI_t
\end{bmatrix}
$$

# Finding points of interest

For this course, we'll focus on things that are easy to track.

We want to see:

1. Points where the largest eigenvalue is less than a threshold ($\lambda_1 < \epsilon$)
2. Points near an edge ($\lambda_1 >> \lambda_2$)
3. Corners or patterns ($\lambda_2 > \tau$)

# KLT Algorithm

The KLT algorithm is a feature-based tracking algorithm that uses the Lucas-Kanade Optical Flow algorithm to track points of interest.

The KLT algorithm is defined as follows:

1. **Detection**: Find points of interest
2. **Tracking**: Track points of interest
3. **Update**: Update points of interest

# Covariance Tracking

Covariance tracking is used to capture spatial and statistical information, along with their correlation. It can fuse different types of features, such as location, color, edges, and motion.

A feature vector is a matrix of rows of pixels where each row is denoted as: $f_k = [x, y, R, G, B]$.

The covariance descriptor is an extracted patch of object from an image. We use:

$$
C^{\text{Model}}=\frac{1}{N}\sum_{i=1}^N(f_i-\mu)(f_i-\mu)^T
$$

Where $\mu$ is the mean of the feature vector and $N$ is the number of feature vectors.

# Mean Shift Tracking

Mean shift tracking is a non-parametric feature-based tracking algorithm that uses the mean shift algorithm to track points of interest.

Simplified mean shift algorithm:

1. **Initialization**: Choose a point of interest
2. **Compute the mean**: Compute the mean of the points of interest
3. **Shift the mean**: Shift the mean to the new mean
4. **Repeat**: Repeat steps 2 and 3 until the mean converges
