---
next: "[[3. Operational Semantics]]"
prev: "[[In Progress/Adv. Programming Languages (CSE6341)/1. Course Overview]]"
---
# Formal Languages
Formal languages are the basis for the design and implementation of programming languages. An **alphabet** $T$ is a finite set of symbols and a **language** $L$ is a set of strings such that $L \subseteq T*$. A **string** is a finite sequence of symbols where:

| Symbol     | Definition                                |     |
| ---------- | ----------------------------------------- | --- |
| $T^*$      | The set of all strings over $T$           |     |
| $T^+$      | The set of all non-empty strings over $T$ |     |
| $\epsilon$ | An empty String                           |     |
# Context-free grammars
A **grammar** $G=(N,T,S,P)$ is comprised of:
- The finite set of nonterminal symbols $N$.
- The finite set of terminal symbols $T$ (our alphabet).
- The starting nonterminal symbol $S \in N$.
- The finite set of productions $P$, where $P$ is a rule mapping nonterminal symbols to other nonterminal and terminal symbols.
More specifically, a production $P$ is structured as $x \rightarrow y$, meaning if we have a nonterminal $x$, it can be substituted with $y$. $x$ must be a non-empty sequence of nonterminals and terminals and $y$ might be any combination of other nonterminals and terminals. 

Compilers and interpreters use grammars to process files into machine-readable code. 

A **context-free language** is a strict superset of regular languages defined above. Unlike regular grammars, a [[2. Formal Languages#Context-Free Languages|context-free grammar]] requires that each production $A\rightarrow w$ follow a stricter rule set, where $A$ must be one single nonterminal instead of a sequence.

An alternative notation for writing productions is the form `<A> ::= <w>`, called the Backus-Naur form.

## Derivation Trees
A **parse tree** (also known as a derivation tree or concrete syntax tree) is a tree of expanded productions. In one, leaf nodes represent terminals, inner nodes represent nonterminals, the root is the starting nonterminal, and the string is all of the leaf nodes from left to right. 

Instead of a concrete syntax tree, we can use an **abstract** syntax tree, which does not capture any unnecessary syntax. This is what we use in the projects.

### Ambiguity
When a string has multiple possible parse trees, it is **ambiguous**. Since we want our parser to produce consistent results, we need to remove ambiguity. We can do so by:
1. Changing the grammar.
2. Giving the parser rules of evaluation to add priority.
A primary example of ambiguity is binary math operators. When these operators are commutative or associative, we can evaluate them in different orders while still getting the same results. We can give the parser precedence rules to evaluate the left or right side of a binary operator to eliminate ambiguity. 

Alternatively, we can define the grammar to force parenthesis. This is too much work for the programmer, though, so instead, we create more nonterminals that implicitly create precedence rules, such as below. 
![[Screenshot 2024-10-06 at 3.17.01 PM.png]]

A **left-associative** operator evaluates the left operand first, meanwhile a **right-associative** operator does the opposite. 

# Attribute grammars
Context-free grammars struggle to represent semantics because most of the time, our programs rely on context-sensitive conditions. This is why we use **attribute grammars**, a generalization of context-free grammars, for tasks such as semantic checking and compile-time analyses. 

Attribute grammars are the addition of attributes to terminals and nonterminals that allow us to enforce conditions and even perform some computations without actually running the program. In other words, an attribute grammar is made of:
- An underlying context-free grammar.
- Some attributes, which are assigned to terminals and nonterminals.
- Possible types of each attribute.
- A set of evaluation rules for each production.
- A set of boolean conditions for attribute values.

A **valid** parse tree is one where all the attribute values are consistent with its evaluation rules and all boolean conditions are true. Both the a) evaluation rules and b) conditions must be evaluated to be true. 

There are still some constraints, though. If a nonterminal has an attribute, each occurrence of that attribute must always have that attribute. Additionally, an attribute cannot refer to itself or a grandparent or grandchild. Values must be referred to in the immediate vicinity. 

### Examples
$L=\{a^nb^nc^n|n>0\}$ produces a language containing $\{\text{abc},\text{aabbcc},\text{aaabbbccc},...\}$. It has the following attributes:
- $Na$, associated with \<A>
- $Nb$, associated with \<B>
- $Nc$, associated with \<C>

```
<start> ::= <A><B><C>
	Cond: [<A>.Na == <B>.Nb == <C>.Nc]
<A> ::= a
	<A>.Na := 1
<A> ::= a <A_2>
	<A>.Na := 1 + <A_2>.Na
(Repeated for <B> and <C>)
```

We have two ways to share information between attributes. A **synthesized** attribute uses information from the attributes of its children and **inherited** attributes take information from attributes in previous siblings. For convenience, we assume all terminals have a `lexval` attribute that holds a predefined value. 

```
<start> ::= <A><B><C>
	<B>.expectedNb := <A>.Na
	<C>.expectedNc := <A>.Na
<A> ::= a
	<A>.Na := 1
<A> ::= a <A_2>
	<A>.Na := 1 + <A_2>.Na
<B> ::= b
	Cond: [<b>.expectedNb=1]
<B> ::= b <B_2>
	<B_2>.expectedNb := <B>.expectedNb - 1
(Repeated for <C>)
```

In the above example, `Na` is synthesized because it uses information from its children, and `expectedNb/Nc` are inherited because they use information from `Na`, a previous sibling. 

### Dependence Graphs
A dependence graph is a graph of attributes where $x \mapsto y$ means attribute $x$ is used by $y$ to synthesize or inherit a value. If `<X>.A := <Y>.B`, then $Y.B \mapsto X.A$. This is useful for finding an algorithm for evaluating a parse tree.

1. Build dependence graph.
2. Topologically sort the graph.
3. Evaluate in sorted order if there are no cycles.


We can also build a dependence graph based on attribute evaluations. If `x1.A:=x2.A`, then $x_2.A \rightarrow x_1.A$. 
While there can be cycles in our dependence graphs, for this course we disallow cycles, meaning we only have to deal with directed acyclic graphs. 

## Use cases
### Type checking
One such use for attribute grammar is **type-checking**. Using a synthesized type attribute to collect identifiers and an inherited table attribute, we can assert a language's rules for types. This is not very efficient, though, and there are better ways to do this.

We can also implicitly check some aspects of the program, such as divide-by-zeros. 

### Side effects
Pure attribute grammars are expensive because each node has its own set of local data. It is slower, but it is easier to think about and we don't worry about updates to shared data. When we use global data structures for attribute grammars, we introduce side effects alongside our gained efficiency. Because we are sharing data, some actions may cause side effects. 

Considering side effects, we need to add additional restrictions on our evaluation instead of using any topological sort order. We can treat these as artificial edges in the dependence graph. 

### Assembly code generation
Another use for attribute grammars is code generation. Given two sets of language rules, if they can map to each other, we can create that attribute grammar that translates between the two. 
- Assembly code is generated by traversing the parse tree and emitting instructions.
- For example, an addition operation might generate two load instructions followed by an add instruction.
- Control flow constructs like `if` and `while` generate branch instructions in the assembly code.
- Labels are used to mark the start and end of blocks of code for branching.
