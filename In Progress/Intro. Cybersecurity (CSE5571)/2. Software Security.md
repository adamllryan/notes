**Software security** is the idea of engineering software such that it continues to function correctly under malicious attack. If a bug is present, we want to ensure that our software is able to run despite these issues. 

Security holes are common in software, and they are becoming more commonly exploited. The **Trinity of Trouble** states that:
- growing connectivity via the Internet increases the number of ways to attack a system. 
- increasing the extensibility of systems, meaning systems accept updates or extensions such that the functionality of the system is evolved in an incremental fashion.
- a growing size and complexity of operating systems. 

# Software Vulnerabilities
A **software vulnerability** is a security related software defect that can be exploited by someone to have an undesirable effect. **Software defects** occur when a piece of software behaves incorrectly or fails to meet requirements, and can happen due to:
- incorrect design, known as a flaw.
- incorrect implementation, known as a bug.

It's hard to prevent all bugs and flaws, so we can aim to prevent or reduce them in the first place. **Software design** and the software development lifecycle are the steps and phases that comprise a framework for designing and maintaining software. 

**Waterfall** and **Agile** models are the most common. Depending on the project size, other models may be used. 

Software security requirements should be derived from business needs, mission, or other objectives. Internal security policies are typically used. For example, HIPAA may be a regulatory policy governing software development in healthcare. 

We identify these requirements via a process that may include:
- interviewing stakeholders
- identifying applicable policies/standards
- finding and mapping regulatory, compliance, and privacy considerations
- conducting an initial risk assessment
An SDLC becomes **secure** when we add security-related activities to it. 

## Memory Errors
### Dangling pointer
Also known as use-after-free, this occurs when a pointer is freed but the reference still exists. 

### Format string vulnerabilities
If we call `printf` without a format string at the start, an attacker can inject a format string and modify behavior. 

## Fundamental Software Security Principles
The **fundamental software security principles** list is a set of high-level security principles for building secure software. It applies to security in general, may conflict (must find tradeoffs based on risk), and is a decision-making guide for software choices during development, maintenance, and assessment. It was originally eight principles but has since added two more. 

### Economy of mechanism
This first principle, also known as **reduced complexity**, simply states that a simpler design is easier to test and validate. 
### Fail-safe defaults
Also known as **protective defaults**, this principle says all systems will fail, so we should include failure states to handle failures when they occur. The safe default is generally no-access, meaning authorization should be denied by default. 
### Complete mediation
This principle says access rights should be completely validated for every access. The authorization system should never be circumvented, even with repeated accesses. This is a tradeoff between security, efficiency, and convenience for human users. 
### Open design
Open design states that the security of a system must not rely on obscure design. 
### Separation of privilege
A protection mechanism is more flexible if it requires more than a single entity to complete it. This ensures no single entity can abuse the system. 
### Least privilege
Every software element or user is allocated privileges that are necessary to accomplish its specified functions but no more. 
### Least common mechanism
Also known as **least sharing**, system mechanisms should not be shared unless absolutely necessary because shared mechanisms might provide unintended communication paths or means of interference. 
### Psychological acceptability
Psychological acceptability, also known as **least astonishment**, recognizes that humans are a key part of software, so security should be designed to reflect the user's mental model of protection. Users will not use these protections if they don't understand them or if it burdens them. 

# Exploits
## Buffer Overflow
A **buffer overflow** occurs when a program is allowed to write beyond the end of an allocated buffer or array. 

While high-level/interpreted languages or memory-safe languages do not suffer from these, languages such as Asm and C/C++ allow direct memory access or buffer overflow. While it comes with the best performance and efficiency, it has the cost of leaving user data and program flow control information intertwined, allowing attackers to cause harm through hijacks such as stack smashing. 

When we execute a program, it creates a process. This **process** has an OS virtually managed memory space consisting of different segments. These segments contain the program's machine instructions, global variables, and local variables. It also has access to dynamically allocated heap segments. 

A **stack pointer** points to the top of the stack and moves the pointer down to the next address. It starts at the highest address and moves toward the bottom as more blocks are used. A **frame pointer** (base pointer) points to the base address of the stack, the **stack pointer** points to the base address of the stack and the **instruction pointer** points to the address of the next instruction. 

Since there is no automatic register cleanup, it is convention for caller functions to push call arguments onto the stack and then restore them afterward. The called function pushes other registers onto the stack and is responsible for replacing them afterward. 

Using this, the attacker can use a buffer overflow attack to change the return address pointer to point to some injected code of their choosing. 

## Control Hijacking
Control hijacking, or stack smashing, attacks exploit a buffer overflow to inject code into the stack and manipulate `%eip` (on 64-bit `%rip`) to execute it. If the attacker just replaces the return address with the injected code, it will crash afterward because there the operating system will not know where to go post-execution. Instead, the attacker saves the correct return address, replaces it with the injected code address, and then restores it before the injected code terminates. 
# Defenses
The best prevention is to not use memory-unsafe languages. However, if using them is necessary, we can use the following safeties:
- do not use unsafe functions
- enforce bounds checking (input validation, etc)
- stack canaries (detect return address manipulation)
- use non-executable (nx) memory managed by hardware (W^X policy)

Control hijacking is still possible, though because the attacker just focuses on finding a system call they want and then calling it through manipulating the return address. 

### Address Space Layout Randomization
When the address space layout is deterministic, anyone with access to the binary file can predict target addresses. The idea of **ASLR** is to randomize the base address of different memory sections (stack, heap, executable memory, libraries). You can still reverse-engineer this, though. 

**Common Vulnerabilities and Exposures** (CVE) is a database of publicly disclosed cybersecurity vulnerabilities. Zero-day vulnerabilities are ones only known to the attacker, meaning we do not have any ready defenses against them. 

**Common Weakness Enumeration** (CWE) is a list of common software and hardware weaknesses related to memory access. 

**Memory safety** is the correct use of memory in space and time. Spatial safety refers to a pointer only accessing memory that it owns and temporal safety refers to accessing only memory that exists and is ready to use. This can apply to a program or a whole programming language. 

# Secure Software Development

## Dynamic Testing
We distinguish between **black-box** and **white-box** testing based on if we can debug and see the internal code. 

## Code Review
Code review is having another person who did not work on the code review look for potential issues. We can automate this via **static code (security) analysis**. However, it cannot replace all testing because it cannot detect buffer overflow every time. Static analysis may also produce false positives and false negatives. 

One way we can use this is to perform taint analysis, which identifies variables that are related to ones that involve user-controllable input (tainted variables). The **defense-in-depth principle** is the application of multiple countermeasures in a layered manner to achieve security objectives. 

## Symbolic Execution
**Symbolic execution** is an attempt to generalize testing. We treat user input as a math symbol and run through it to see if any issues might occur. 

## Penetration Testing
**Pen testing** is a methodology where assessors try every available method to try and break a system and its security features. It tests the whole system, not just the software. 

## Fuzzing
**Fuzzing** is a technique that uses random input to improve test coverage. Some techniques it might employ are:
- mutation, where you take legal input and change it
- generational, which is random or based on a grammar.
- combinations








