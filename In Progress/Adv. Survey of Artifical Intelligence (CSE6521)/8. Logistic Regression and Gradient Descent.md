---
completed: false
next:
prev:
---

# Linear Regression

Linear regression is a simple model that assumes a linear relationship between the input features and the output. It is a simple model that is easy to understand and implement. It is a good starting point for understanding more complex models.

It uses the equation:

$$
y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

where:

- $y$ is the output
- $w_0$ is the bias term
- $w_1, w_2, ..., w_n$ are the weights
- $x_1, x_2, ..., x_n$ are the input features

We can always find an optimal set of weights that minimizes the error between the predicted output and the actual output. However, an answer like this is often an outlier and it may be much more difficult to get a correct solution.

# Gradient Descent

Gradient descent is the solution to these problems where linear regression is not appropriate. It is an optimization algorithm that is used to minimize the error between the predicted output and the actual output. It works by iteratively updating the weights in the direction of the steepest descent of the error function.

The algorithm works as follows:

For any point in the parameter space $w$, while not converged do: for each $w_i$ in $w$ do $w_i = w_i - \alpha \frac{\partial E}{\partial w_i}$

where:

- $E$ is the error function
- $\alpha$ is the learning rate
- $\frac{\partial E}{\partial w_i}$ is the partial derivative of the error function with respect to the $i$-th weight
