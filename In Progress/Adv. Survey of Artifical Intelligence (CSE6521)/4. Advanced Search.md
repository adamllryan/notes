---
completed: true
next: "[[5. Machine Learning and Decision Trees]]"
prev: "[[3. Search]]"
---
We can use [[02_uninformed_search#Graph Search|graph search]] to solve the issues linked to tree search in a non-DAG system. We can prevent cycles by keeping track (via a set) of visited nodes and stopping iteration if we hit an already visited node. When using A* search, it is optimal if the heuristic is consistent, not admissible. 

# Local Search and Optimization
Local search departs from the algorithms prior because it only searches neighboring states for its next move. Typically, it is not optimal, but it comes with a few advantages. It uses less memory and is useful because it can typically find a reasonable solution in large or infinite state spaces. This will usually try to optimize some utility function of the state. 

## State-space landscape
**Hill-climbing** (gradient ascent) is the goal of finding a global maximum, and **gradient descent** is the process of finding the global minimum. Hill-climbing always moves to the neighboring state with the highest value (steepest ascent) and terminates when no neighbor has a higher value. 

## Greedy Local Search
The greedy local search algorithm takes the greedy solution around neighbors and often gets stuck. We can use **simulated annealing** as a way to escape local maxima and minima by "shaking" the current agent spot, which means moving the cursor to a random state to hopefully free it from the min or max. 

## Local Beam Search
Local beam search uses hill-climbing using a queue of size $k$. It generates $k$ random positions and then uses greedy local search, terminating if a goal state is found. Only the best $k$ is kept per iteration, unless we want to use simulated annealing, where we keep a random $k$. 

## Genetic & Evolutionary Algorithms
Genetic/evolutionary algorithms are inspired by real-world evolution, using metaphors from genetics to help generate new candidates. For example, a stochastic beam search implementation that uses two parents to generate new examples instead of one. Another example is "survival of the fittest", which uses rankings to help determine which candidates to keep. 

## Continuous Gradient Descent
Within a continuous space, we can perform gradient descent by computing a loss function and stepping in the direction that minimizes loss (negative gradient). 