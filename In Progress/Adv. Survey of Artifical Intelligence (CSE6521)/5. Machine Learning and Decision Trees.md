---
completed: false
next: "[[5. Non-Parametric Machine Learning]]"
prev: "[[4. Advanced Search]]"
---
Machine learning is the use of agents that learn to improve behavior through the study of past experiences to make predictions. Through its study, an agent develops a model of the world on its own. 

Supervised learning takes input-output pairs and learns a function that maps inputs to outputs. 
Unsupervised learning learns patterns from the input without any explicit feedback. 
Self-supervised learning automatically solicits feedback from data without human annotation.
Reinforcement learning learns from a series of reinforcements, such as rewards and punishments. 

Classification is where the output is comprised of a finite set of values. 
Regression is where the output is a continuous value. 

# Supervised learning
Given a training set of $N$ input-output pairs, supervised learning learns a function that approximates the expected output given input. 

**Consistency** is the fitness of hypothesis to data.
**Generalization** is performance of the model on a separate test set. 
**Underfitting** and **overfitting** refer to the model being too little or too much dependent on the training set. 
**Bias** is the predicted hypothesis systematically deviating from data.
**Variance** is the change of hypothesis due to fluctuation in training data.
**Capacity** is the rough size of the hypothesis space. 
**Bias-variance tradeoff** and **Ockham's razor**

# Decision tree induction
Decision tree induction is a greedy algorithm that builds a tree in a top-down, recursive, divide-and-conquer manner. All of teh training examples begin at the root adn examples are partitioned recursively based on selected attributes. Partitioning is stopped when:
- all examples for a given node belong to the same class
- there are no remaining attributes for further partitioning (majority voting employed)
- there are no examples left

It is implemented the following way:
1. For each value of $A$ where $A$ is the best attribute for splitting tuples, craete new child nodes
2. for each child node or subset
3. if one of stop conditions met, stop. Otherwise split(child, subset)

# Entropy
SLIDE 13
