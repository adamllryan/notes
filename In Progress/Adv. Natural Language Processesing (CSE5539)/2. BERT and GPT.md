---
completed: false
next: 
prev: "[[In Progress/Adv. Natural Language Processesing (CSE5539)/1. Introduction|1. Introduction]]"
---
# Self-Attention
Take a word. 
Do an embedding lookup and multiply against query, key, and value vectors to produce attention weights. 

# Position Embeddings
Self-attention does not handle the position of words in a sentence. It is proposed to add positional embeddings to 

Uses sinusoidal embeddings to encode relative positions. This way you can subtract any two embeddings and have the same value if they are the same distance apart. 