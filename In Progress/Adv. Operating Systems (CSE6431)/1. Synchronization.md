---
completed: false
next: "[[2. Distributed Systems]]"
prev:
---

# Process
A [[2. CPU Scheduling#Process|process]] is a program whose execution is in progress. This is a simplified set of states for a process:

![[Screenshot 2024-08-22 at 12.51.37 PM.png|500]]

## Process Memory Space
PMS contains the code, data, stack, and heap segments. 

| Space | Function                                            |
| ----- | --------------------------------------------------- |
| Code  | Contains static code                                |
| Data  | Stores global variables                             |
| Stack | Contains local variables of functions               |
| Heap  | Contains dynamically allocated memory (malloc/free) |

# Sharing of Memory Space
Multiple processes share nothing by default. Within the same process, each thread shares the code, data, and heap segments but does not share stacks. 

Typically, we aim to have our program act like a single [[8. Concurrency and Parallelism#Atomicity|atomic]] set of instructions. When two or more threads access shared data without synchronization, we consider this a [[8. Concurrency and Parallelism#Data Race|data race]]. 

# Locks
We can use [[17. Process Synchronization#Synchronization Hardware|locks]] to restrict write (and sometimes read) access to a variable or piece of memory. We create a lock that only one thread can acquire at a time. We consider areas that need locks to be **critical sections**. 

For this course, we believe that the lock is intended to protect the shared variable, not the piece of code (though this has been controversial throughout history). The **all-or-nothing** property states that all code touching a shared variable must be protected by its corresponding lock. 

## Resolving Locking Issues
We cannot natively implement locks in software, so we must rely on hardware-implemented locks. Since different hardware vendors provide different locking methods, we should only use lock libraries that handle hardware locks for us. 

**Spinning** is the idea of waiting in a while loop until a lock is released. This generates overhead because we are wasting CPU cycles just checking and waiting. A simple solution to reduce wasted cycles is to call `yield()` (places into `Ready` state), which gives up the remaining CPU cycles manually. 

Modern-day operating systems provide calls such as `park()` and `unpark()` to pause threads (place them into `Blocked` state) and reduce wasted cycles. 


A better implementation works as

```
typedef struct __lock_t {
	int flag; // whether the lock is locked
	int guard; // protects flag and queue (secondary lock)
	queue_t *q; // queue of blocked processes
} lock_t;

void lock_init (lock_t *m) {
	m->flag = 0;
	m->guard = 0;
	queue_init(m->q);
}

void lock(lock_t *m) {
	while (TestAndSet(&m->guard, 1) == 1); // guard lock to protect vars
	if (m->flag == 0) {
		m->flag = 1; // lock the lock
		m->guard = 0; // release guard lock
	} else {
		queue_add(m->q, gettid()); // wait in queue
		m->guard = 0; // release guard
		park(); // block state
	}
}
void unlock(lock_t *m) {
	while (TestAndSet(&m->guard, 1) == 1); // guard lock to protect vars
	if (queue_empty(m->q))
		m->flag = 0; // release lock
	else
		unpark(queue_remove(m->q)); // release next thread from blocked
	m->guard = 0; // release guard lock
}
```

If there is no contention, it is better to use spin locks due to the time it takes to execute system calls. **ALSO**: parking after releasing the guard is dangerous, we use a special OS-provided `park()` that executes after a few instructions so that we can keep `park()` within the critical section. 

## Thread Safe Data Structures
Most implementations of data structures are not **thread-safe**, meaning they are not protected or designed for data concurrency. You should check that they are thread-safe before using them in threaded applications. 

### Counters
A **sloppy counter** is a parallelized counter implementation. Each CPU has its own local counter protected by local locks and syncs with a global counter, also protected by a global lock. When updating, update the local counter and update the change with the global counter. When reading, just read the global counter. This approach is very fast but may not be very correct. 

### Linked Lists
The concurrent implementation of linked lists comes with a non-negligible overhead because each node now has its own lock. 

### Queues
A concurrent queue uses one lock for the head and one lock for the tail. There is a better approach covered later. 

### Hash Table
A concurrent hash table is pretty useful. 

# Semaphore
A **semaphore**, proposed by Dijkstra, was the first high-level construct used to synchronize concurrent processes. A semaphore $S$ is an integer variable on which two atomic operations are defined, $P(S)$ and $V(S)$. The $P$ operation may block a process but $V$ does not. 

```
P(S): S := S - 1;
if S < 0 <block and enqueue the process>;  
V(S): S := S + 1;
if S <= 0 <dequeue and unblock first process>;
```

There are two types of semaphores. A **binary semaphore** has an initial value of 1 and a **resource-counting semaphore** uses any initial value. 

# Classical Synchronization Problems
## Producer-Consumer Problem
Assume we have a finite buffer pool. A producer adds items to the pool and consumer removes from the pool. The producer waits if the buffer is full and the consumer waits if it is empty. 

```C
//Producer
P(sem_p);
P(mutex);
increment();
V(mutex);
V(sem_c);
```

```C
// Consumer
P(sem_c);
P(mutex);
decrement();
V(mutex);
V(sem_p);
```

## Reader-Writers Problem
Similar to the producer-consumer problem, now readers should be able to concurrently access the resource. Only readers can access data in parallel, if one writer writes, then everyone else is locked out. 

```C
// Reader
P(reader_mutex);
if (readers == 0) {
	P(writer_mutex);
}
readers += 1;
V(reader_mutex);
read_file();
P(reader_mutex);
reader -= 1;
if (readers == 0) {
	V(writer_mutex);
}
V(reader_mutex);
```

```C
// Writer
P(writer_mutex);
write_file();
V(writer_mutex);
```

### Reader-Writer with Reader Priority
```C
// Reader
P(reader_mutex);
if (readers == 0) {
	P(writer_mutex);
}
readers += 1;
V(reader_mutex);

read_file();

P(reader_mutex);
readers -= 1;
if (readers == 0) {
	V(writer_mutex);
}
V(reader_mutex);
```

```C
// Writer
P(sr_mutex);
P(writer_mutex);

write_file();

V(writer_mutex);
V(sr_mutex);
```

## Dining Philosophers Problem
We have 5 philosophers at a circular table and one fork in between each (5 forks). Each philosopher is either thinking or eating. Each needs two forks at a time to eat, meaning all cannot be eating at the same time. 

```C
#define N 5;
#define LEFT (i+N-1)%N;
#define RIGHT (i+1)%N;
#define THINKING 0;
#define HUNGRY 1;
#define EATING 2;
typedef int semaphore;
int state[N];
semaphore mutex = 1;
semaphore s[N];

void take_forks(int i) {
	down(&mutex);
	state[i] = HUNGRY;
	test(i);
	up(&mutex);
	down(&s[i]);
}

void put_forks(int i) {
	down(&mutex);
	state[i] = THINKING;
	test(LEFT);
	test(RIGHT);
	up(&mutex);
}

void philosopher(int i) {
	while (TRUE) {
		think();
		take_forks(i);
		eat();
		put_forks(i);
	}
}

```

# Condition Variable
**Condition variables** (CV) are variables that define two operations: `wait()` and `signal()`, used to delay and resume the execution of processes. Each condition variable stores a queue that returns true if not empty. Calling a CV's `wait()` will suspend the process and enqueue onto its queue. Calling that CV's `signal()` wakes up the first process on the queue and resumes execution. 

A semaphore knows if `p()` has been called after a `v()` call, meanwhile, a condition variable's `signal()` will be lost if called before a `wait()`. 

Textbooks often assume that waiting threads will not wake up unless `signal()` is called, but in actual implementations in Linux and Windows, spurious wakeups may occur and wake up processes without `signal()` being called. 

# Monitor
**Monitors** are an abstract data type that encapsulates shared resources. It consists of shared objects and variables and a set of procedures. 

The only operations allowed are procedures that can be performed on the resource and local variables. Only one process at a time can be active within a 


## Monitors in Synchronization

Monitors in synchronization are a high-level abstraction used in concurrent programming to manage access to shared resources by multiple threads or processes. A monitor combines **mutual exclusion** and **condition synchronization** to ensure that only one thread can execute a critical section at any given time, while also allowing threads to wait and signal each other based on certain conditions.

### Key Concepts of Monitors:
1. **Mutual Exclusion (Mutex)**: 
   - Only one thread can be active in the monitor at a time. When a thread enters a monitor (i.e., when it starts executing one of the monitor's procedures or methods), other threads attempting to enter will be blocked until the current thread exits the monitor. This ensures that shared resources are not accessed concurrently, preventing race conditions.

2. **Condition Variables**:
   - Monitors often use **condition variables** to manage synchronization between threads. A condition variable allows a thread to wait (suspend execution) until a particular condition is true and to be notified (resumed) when another thread signals that the condition has changed.
   - Two common operations on condition variables:
     - **Wait**: A thread that needs to wait for a condition will invoke the `wait()` method. This releases the monitor (so other threads can enter) and puts the waiting thread to sleep.
     - **Signal**: When a thread finishes modifying the shared resource and changes the condition, it can invoke `signal()` to wake up one of the threads waiting on that condition.

### Example of Monitor Usage:
In pseudo-code, a monitor might look like this for managing access to a shared buffer:

```java
monitor SharedBuffer {
    condition notEmpty;
    condition notFull;
    int buffer[];

    procedure addItem(item) {
        if (buffer is full) {
            wait(notFull);  // Wait until there's space in the buffer
        }
        add item to buffer;
        signal(notEmpty);  // Notify waiting consumers that buffer is not empty
    }

    procedure removeItem() {
        if (buffer is empty) {
            wait(notEmpty);  // Wait until there's an item in the buffer
        }
        remove item from buffer;
        signal(notFull);  // Notify waiting producers that buffer has space
    }
}
```
### Benefits of Monitors:

- **Encapsulation**: Monitors encapsulate both the data (shared resources) and the synchronization code (conditions and mutexes), leading to cleaner and safer code.
- **Ease of Use**: By providing high-level abstractions like condition variables, monitors reduce the risk of low-level synchronization errors (e.g., deadlocks, race conditions).

### Drawbacks:

- **Performance Overhead**: Since monitors enforce mutual exclusion, they can reduce parallelism, especially in cases where multiple threads could operate concurrently without conflicts.
- **Complexity**: While monitors simplify synchronization, complex interactions between threads might still lead to issues like priority inversion or deadlock if not carefully managed.

In summary, monitors are a synchronization mechanism that help control thread access to shared resources by combining mutual exclusion and condition synchronization, providing a structured way to prevent concurrency issues.