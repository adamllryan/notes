## Introduction
A **Boltzmann machine** is a stochastic learning machine consisting of visible and hidden units linked by symmetric (two-way) connections. They use **stochastic** neurons with sigmoid activation functions where the state of the neuron is:

$$
x_i=
\begin{cases}
1 & \text{ if probability } \phi(v_i)\\
-1 & \text{ if probability } 1-\phi(v_i)
\end{cases}\\
\text{ where }
 \phi(v)=\frac{1}{1+e^{-\frac{2v}{T}}}
 
$$

The goal of Boltzmann learning is to produce a network that can model the probability distribution of input patterns (visible variables). It should be able to compute the probability of the next observation, learn the model's parameters from data, and estimate likely values given partial observations. It is very similar to **statistical machines**. In other words, it should take visible data and be able to give us the probabilities of each event occurring as a combination of input patterns. 
![[Boltzmann Machine.png]]
## Energy
Boltzmann machines use **energy** to produce outputs. The energy function for a vector $x$ containing states for each neuron is:

$$
E(x)=-\frac{1}{2}\sum_i\sum_jw_{ji}x_ix_j=-\frac{1}{2}x^Twx
$$
Since we have symmetric connections between neurons $i$ and $j$, we know that $w_{ij}=w_{ji}$. Also, $w_{ii}$ always will be $0$ for all $i$. 

## Boltzmann-Gibbs Distribution
Given energy at state $i$ as $E_i$, probability $p_i$, and temperature $T$, we get:

$$
p_i=\frac{1}{Z}e^{-\frac{E_i}{T}}\text{ where } Z=\sum_ie^{-\frac{E_i}{T}}
$$

Above, $Z$ is called our **partition function**. The above formula is known as the **Boltzmann-Gibbs distribution**. We can see that lower energy states have a higher probability of occurring. As T decreases, the probability is concentrated on a smaller subset of low-energy states. 

## Calculating the Next Observation
To compute the probability of a new observation, we start by dividing the network into $x_\alpha$ visible units and $x_b$ hidden units. Given parameters $w$ (weights), we compute $P(x_\alpha)$ with:

$$
P(x_\alpha)=\sum_{x_\beta}p(x)=\sum_{x_\beta}\frac{1}{Z}e^{-\frac{E(x)}{T}}=\sum_{x_\beta}\frac{1}{Z}e^{\frac{1}{Z}e^{-\frac{E(x)}{T}}}=\sum_{x_\beta}\frac{1}{Z}e^{\frac{1}{Z}x^Twx}
$$

## Learning
Boltzmann machines learn using the log-likelihood of data. We attempt to maximize the likelihood of the visible units that take training patterns. Assuming each pattern is independent, the log probability of the training sample is:

$$
L(w)=\sum_{x_\alpha}[\log\sum_{x_\beta}e^{-\frac{E(x)}{T}}-\log\sum_xe^{-\frac{E(x)}{T}}]
$$

Therefore, the gradient of $L(w)$ is

$$
\frac{\partial L(w)}{\partial w_{ji}}=\frac{1}{T}a(p^+_{ji}-p^-_{ji})
$$

where $p^+_{ji}=\sum_{x_\alpha}\sum_{x_\beta}P(x_\beta|x_\alpha)x_jx_i=\sum_{x_\alpha}E_{x_\beta|x_\alpha}{x_jx_i}$ and $p^i_{ji}=K\sum_xp(x)x_jx_i\approx E_x{x_jx_i}=<x_jx_i>$ and $K$ is the number of visible neurons.

## Maximizing $L(w)$
To maximize $L(w)$, use gradient descent. The learning phase has two parts:

- Positive phase, where the network operates in the clamped condition (visible neurons take on training patterns).
- Negative phase, where the network operates freely without the influence of external input.

The high-level idea is to learn the distribution and the features of that distribution.

## Problems
Despite being a simple learning rule, computing $p_{ji}^+$ requires massive computations ($2^H$ term summations). As a result, we try to use approximations of these methods. Due to its computational complexity, Boltzmann machines have only been applied to toy problems. 

## Sampling
**Sampling** is a solution to reduce complexity. It approximates our expectations by choosing a set of samples from our total probability distribution. 

$$
p^-_{ji}=E_x\{x_ji_i\}\approx\frac{1}{N}\sum_{x_n}x_jx_i
$$
where $x_n$ are samples drawn from $P(x_n)$. In general, $E_x\{f(x)\}\approx\frac{1}{N}\sum_{x_n}f(x_n)$. The sum converges to the true expectation as we increase the number of samples. This is known as **Monte Carlo Integration (Summation)**. 

**Gibbs Sampling** makes drawing samples more efficient by using conditional probabilities. It depends on being able to compute the probability of variables given other probabilities. This type of approach is known as **Markov Chain Monte Carlo**. For Boltzmann machines, each step of the Gibbs sampling algorithm corresponds to one stochastic neuron, denoted as:

$$
P(x_i \rightarrow -x_i)=\frac{1}{1+e^\frac{\Delta E_i}{T}}
$$

## Simulated Annealing
As our temperature $T$ decreases, we see that the average energy of a stochastic system tends to decrease. It reaches the global minimum as $T$ approaches $0$. For optimization problems, this is what we want. However, as $T decreases, convergence becomes much slower. **Simulated annealing** gradually decreases $T$ so that we can have the best of both worlds. 

Simulated annealing does not guarantee a global minimum, but has higher chances for getting local minima. 

## Use in MLPs
Deep networks use more than one hidden layer, meaning that we can deal with vanishing gradients. We can fix this by **pretraining** our network, which is a method to initialize deep neural networks. 

**Unsupervised pretraining** is a form of pretraining for initializing deep neural networks. The idea of this is to train a super basic and incorrect version of this model that is bad but kind of gets the idea. We start with a shallow network (one hidden layer) and train for a bit. We freeze the weights and add another hidden layer. We train only this final layer and then repeat this until we reach our desired network structure. Afterward, we begin to fully train the model. This pretraining process effectively gives us a blueprint of a model that the network will feed off of to settle in a good state. 

Unsupervised learning is great for this task because we expect it to not overfit. Since there are no labels, it is harder to overfit and it is better at generalizing. It also allows us to validate what the network has learned so far. 

## Restricted Boltzmann Machines
A **restricted Boltzmann machine** is a Boltzmann machine with one visible layer and one hidden layer. Unlike regular Boltzmann machines, these are **restricted**, meaning they are not allowed to have connections between the same type of neurons (i.e. hidden to hidden or visible to visible). 

![[Restricted Boltzmann Machine.png]]

## Forward and Backward Pass
Initially, start by setting visible neurons $v_i$ to the input ($v_i=x_i$). Then, the activation for each hidden neuron becomes $\phi(\sum_iw_{ji}v_i)\equiv P(h_j=1|v)$. 

The **backward pass** attempts to learn to reconstruct the data. The activations from the hidden layer become inputs to the visible layer. Each of the weights are the same, and we compute the activation for each visible neuron $i$: $P(v_i=1|h)=\phi(\sum_iw_{ji}h_i)$, which closely approximates our original input. 

We can simplify our definitions of both passes by describing them in terms of visible $v$ and hidden neurons $h$. We are left with:

$$
P(h|v)=\phi(w^Tv) \text{ and } P(v|h)=\phi(h^Tw)
$$

Initially, $v=x$ but is updated in the backward pass. 

## RBM Energy
The energy function for an RBM is:

$$
E(v,h)=-\sum_i\sum_jw_{ji}v_ih_j
$$

## RBM Gradient
This time, we do not have a temperature $T$. So for our gradient, we substitute $T$ as $1$. 

$$
\frac{\partial L(w)}{\partial w_{ji}}=\frac{1}{T}\\(p_{ji}^+-p_{ji}^-)
\text{ where }
p_{ji}^+=\sum_v{\sum_h{P(h|v)h_jv_i}}
$$
$$
\text{ and }
p_{ji}^-=\sum_v\sum_{v,h}P(v,h)h_jv_i
$$
$p_{ji}^-$ is still difficult to compute, so we use Gibbs sampling to approximate, again. 

## RBM Learning
To find the true gradient, we need to infinitely perform the forward and backward pass. Since this is not possible, we settle for **contrastive divergence**. This alternative starts with the input, samples $h$, then $v$, and then $h$ again. The weight change is calculated by taking $\Delta w_{ij}=<h_i^{(0)}v_j^{(0)}>-<h_i^{(1)}v_j^{(1)}>$. 

| **Previous**: [[16. Self-Organizing Maps]] | **Next**: [[18. Self Supervised Learning]] |
| ------------------------------------------ | ------------------------------------------ |