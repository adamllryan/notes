---
id: 10. Softmax Layers
aliases:
  - softmax
tags: 
pass: "3"
---
Suppose we have a classification problem that deals with non-exclusive data (there may be multiple solutions). At the output layer of the network, we are getting a lot of different decimals that don't necessarily represent the confidence of the network that a classification is true. While we could just take the max of the network as our final answer, that means we lose out on a lot of knowledge. 

Say we want to classify if an image has a dog or a cat within the frame. Given an image of a dog next to a cat, the network returns a 0.50 for dog and a 0.49 for cat. In this situation, we want to know that both are very likely, but simply taking the max value loses out on half of the information. 

The activation potential of every output neuron can be thought of as the *estimated, unnormalized log probability* for the class it represents. So, when we want to classify non-exclusive data, we use the **softmax function** to normalize the output of our networks into a *probability distribution*. We call it a softmax layer because it is applied to each output neuron. For each neuron $z_j$, we use:

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
$$
$e^{z_j}$ is simply $e$ taken to our neuron's output $z_j$'s power, and $\sum_{k=1}^Ke^{z_k}$ just means we sum together $e$ to every neuron's output. Simply put, this is just a modified contribution function where each value becomes the exponent of $e$. 

Now, when we take the sum of our outputs (post-softmax), we observe that they add to one and that the decimal value for each class now represents the probability of that classification being true. 

However, because we changed how we represent the values in our network, we also have to adjust our loss function. Instead of trying to get our network outputs to be one, we just want correct values to have a high probability and incorrect values to have a low probability. When using softmax, we use a new loss function called **cross-entropy loss**. The formula is as follows:

$$
E(w) = -\sum_{i=1}^{C} d_i \log(y_i)
$$

where $d$ is the true label, and $y$ is the predicted label.

| **Previous**: [[9. Convolutional Neural Networks]] | **Next**: [[11. Sequential DNNs]] |
| -------------------------------------------------- | --------------------------------- |
