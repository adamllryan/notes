We can build self-organizing maps using artificial neural networks via **self-organizing maps** (SOMs). This architecture is built upon *competitive learning*, where output neurons compete to be activated. Only one output neuron may be activated at any one time and is considered the **winner-takes-all** or **winning** neuron. SOMs intend to learn how to represent the input space with output neurons during training.

Because of the architecture of SOMs, we are using [[08_gaussian_mixture_models#Unsupervised Learning|unsupervised learning]] to train the network and are not training it to learn a certain task. Instead, we want the network weights to model the distribution of the input data, leading to certain weights firing on certain inputs and a set of topologically ordered weights.

This means that clustering, dimensionality reduction, and topic modeling are target use cases for SOMs.

The principal goal of SOMs is to transform an incoming signal pattern into a one or two-dimensional discrete map in a topologically ordered fashion. ![[Learned SOM.png]]

##

We first take a map (or grid) of neurons, each with a **prototype input** or default set of links. Each node in the SOM has a distance function linked to the other neurons that encourages local groups (and not further groups) of neurons to respond to similar input types. Neurons that are close to each other will have prototypes similar to each other, encouraging competition and local cooperation.

A SOM will form a topographic map of the input patterns where the locations of neurons correspond to the statistical features in the input patterns.

![[Islands of Music.png]]

Self-organizing maps bear practical uses:

- Detecting and describing relations between semantic objects.
- Study semantic roles of words in natural languages.
- Data Analysis Visualization
	- Spacing and Position of high-dimensional clusters.
	- Finding nonlinear patterns in data.
	- Insights about the shape of data.

### SOM Learning Algorithm

#### 1. Initialization

First, we must **initialize** the network. Start by randomly initializing the weights of the neurons in each position to prevent prior ordering.

#### 2. Competition

Then, we generate a **competition** by declaring the neuron with the largest response to an input the winner. To find the winner, we take the *inner product* between each weight vector and the input. The winner is the neuron with the largest inner product.

We find the **best-matching criterion** by trying to maximize the inner product, which is equivalent to minimizing the Euclidian distance between two vectors.

#### 3. Cooperation

We induce **cooperation** by telling the winning neuron's neighbors to also respond to similar stimuli based on their distance.

We use **topological neighborhoods** to implement this. This is a function of *lateral distance* $d_{j,i}$ to each neuron that meets two requirements:

1. This neighborhood is symmetric around the maximal point (winning neuron).
2. The amplitude of the neighborhood decreases monotonically with distance.

Typically, we use **Gaussian** neighborhood functions, where for each non-winner neuron $h_{j,i}$ where $i$ is the center and $j$ is a set of encompassing neurons:

$$
h_{j,i}=e^{-\frac{d^2_{j,i}}{2\sigma^2(n)}}
$$

Because $d_{j,i}$ is the Euclidian distance between our $i$ and $j$ neurons, $d_{j,i}=||r_j-r_i||^2$.

In this function, the **effective width** $\sigma$ creates exponential decay for our topological neighborhood. We can accomplish this by using the function $\sigma(n)=\sigma_0 \cdot e^{-\frac{n}{\tau_1}}$ where $\sigma_0$ is the initial value and $\tau_1$ is a time constant. A wide neighborhood function means many neurons will activate, and decreased width will result in fewer neurons activating for a similar input.

We care about symmetry so much because we want the activation to evenly disperse with distance.

#### 4. Update/Adaptation

Finally, we **update weights** by increasing the excited neurons' weights that relate the most to the input pattern.

Unlike supervised learning, we don't have labels to adjust our weights. There is no cost function, so we use the weight update function:

$$
w_j(n+1)=w_j(n)+\eta(n)h_{j,i(x)}(n)[x(n)-w_j(n)]
$$

which effectively reduces the weights of the winning neuron towards the input vector.

We adjust the learning rate $\eta(n)$ with the function $\eta(n)=\eta_0 \cdot e^{-\frac{n}{\tau_2}}$ where $\tau_2$ is another time constant.

We have alternatives to our width and learning rate functions like:

$$
\sigma(n)=\sigma_0 \cdot(1-\frac{n}{N_0})
$$

where $N_0$ is the number of iterations, and:

$$
\eta(n)=\eta_0 \cdot (1-\frac{n}{N_0+K})
$$

where $K$ is another parameter.

The weight adaptation phase occurs in two steps:

1. Self-organizing or ordering phase.
2. Convergence phase.

We covered the self-organizing phase above, and the convergence phase is simply there to:

- Fine-tune the feature map.
- Provide an accurate statistical quantification of the input.

Here is what the feature space of a SOM typically looks like as we train:

![[SOM in 2D.png]]

The resulting feature map will meet four properties. First, the feature map will provide an *approximation of the input space*. It is not exact because we use iterative training and we are projecting our data into two-dimensional space.

Second, the map is *topologically ordered*. This means that any physical space in the map corresponds to a certain domain or group of features.

Third, the map reflects variations in the *density*/statistics of the input distribution. The mapping has a better "resolution" for inputs that are more likely to occur.

Finally, the map provides *feature selection*, which is the ability to select a set of best features for approximating the underlying distribution.

| **Previous**: [[15. Transformers]] | **Next**: [[17. Boltzmann Machines]] |
| ---------------------------------- | ------------------------------------ |