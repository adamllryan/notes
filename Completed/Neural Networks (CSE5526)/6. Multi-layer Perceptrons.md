Multi-layer networks are neuron networks with at least one hidden layer. The input and output layers are not considered hidden layers. Deep neural networks have three or more layers. In counting the total number of layers, we count the output layer and not the input layer because the output layer contains weights and the input layer does not (it only serves to represent the inputs of the network). The outputs of a network are computed layer by layer, which is called **forward passing**. Each layer's output becomes the next layer's input.

## Training

To train a multi-layer neural network, we need to use back-propagation. **Back propagation** is the process of training iteratively, where we train on a sample using forward pass, compute error, and reverse-iterate through the layers to measure the error contribution from each group, and then update the weights based on that contribution, reducing error. $w_k=w_k-\eta\nabla E(w)$.

Take a multi-layer network with one neuron in every layer. Denote, in sequential order, the neurons as: $x_i$, $j$ (with weight $w_{ji}), $k$ (with weight $w_{kj}$), and $y_k$. The error for an iteration $n$ is $E(n)=\frac{1}{2}\sum_k{[d_k(n)-y_k(n)]^2}\newline=\frac{1}{2}\sum_k{[d_k(n)-\phi(\sum_j{w_{kj}y_j})]^2}$.

As a general rule, the iterative weight update equations (based on the loss function) for each hidden layer are $w_{ji}(n)+\eta\delta_j(n)x_i(n)$ where $\delta_j(n)=\phi'(v_j(n))\sum_k{\delta_k(n)w_{kj}(n)}$. The weight update function for the output layer is $w_{kj}(n+1)=w_{kj}(n)+\eta e_k(n)\phi'(v_k)y_k(n)=w_{kj}(n)+\eta\delta_k(n)y_j(n)$

The **generalized $\delta$ rule** is a solution to determine which neurons are most responsible for error. The intuition between this is that for every error value in the output, we take the weights from the output node and then assign blame by scaling the error by the weight of each previous neuron. For example, if we have an error E, the error at the layer is $\frac{w_n}{\sum{w}}$.

For each layer, the delta value at each neuron is the weighted sum of the delta values of the following hidden layers or output layers.

| **Previous**: [[4. Gradient Descent]] | **Next**: [[7. Neural Network Training]] |
| ------------------------------------- | ---------------------------------------- |