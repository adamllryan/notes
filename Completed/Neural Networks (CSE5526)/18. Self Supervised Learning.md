The goal of **self-supervised learning** is to address the problems of:

- wanting a single model to handle a whole task.
- having large quantities of unlabelled data.
- needing the ability to generalize across datasets.

SSL solves these by learning features (**embeddings**) that encode relationships between data, instead of specialized features.

Step 1 of SSL is the pretraining phase. The idea is that the network will learn multiple features from a signal on its own from raw data. It is done iteratively. Step 2 is to use those learned features as inputs to normal models, now in a supervised manner. Pretty much, we are teaching the network to find its own labels and train itself.

We consider this pretrainer the **encoder** because it takes raw data and encodes it into a set of embeddings that the second-stage neural network can convert into a classification or other results.

We categorize SSL into a few categories:

- auto-regressive predictive coding, where the model learns to predict future latent representations of itself.
- contrastive, where the model contrasts representations from pairs of similar and different samples.
- mask-based, where the model learns contextual information by masking portions of the signal.

## Auto-regressive Predictive Coding

This paper begins with a set of random samples of length $n$ containing one positive sample and $n-1$ negative samples. The authors define positive to be samples from the same class and negative to be samples from differing classes. Using this terminology, this means that the developer defines what is negative and positive. Currently, the definition of positive and negative is still up for debate.

The idea of using these two labels for data is to try and minimize the difference between positive samples and maximize the distance between negative and positive. We try to optimize the loss function, known as **info noise-contrastive estimation (InfoNCE)**.

## Contrastive Predictive Coding

**Contrastive predictive coding** learns to create similar representations for similar samples and dissimilar samples for different samples.

It begins by randomly generating two augmentations (distorting our data, for example rotating an image) of the same signal and attempting to maximize the agreement between those using a contrastive loss. Both augmentations are fed into an encoder to extract an initial set of features and then an MLP to extract another set of features. Taking these two, it performs contrastive loss between the features to maximize the agreement.

## Masked-based Predictive Coding

**Masked** prediction learns representations of masked frames based on the information from unmasked frames. The intuition behind this is that unmasked frames will learn information relevant to the masked frames. It is able to learn sequential context and dependencies.

| **Previous**: [[17. Boltzmann Machines]] | **Next**: [[]] |
| ---------------------------------------- | -------------- |
