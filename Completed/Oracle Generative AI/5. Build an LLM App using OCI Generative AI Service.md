Goals of this module:

- Understand RAG concepts
- Explain vector database and semantic search
- Build LangChain models, prompts, memory, and chains
- Build an LLM application using RAG and LangChain
- Trace and evaluate an LLM application
- Deploy an LLM application

## Chatbot Architecture
**LangChain** is a framework for developing applications powered by language models. It features many components, namely LLMs, Prompts, Memory, Chains, Vector Stores, and Document Loaders. LangChain offers mainly two types of models: LLMs and Chat Models. In LangChain, LLMs refer to **pure text completion** models. They take in text and output a string completion. Chat models, on the other hand, are backed by LLMs but are tuned specifically for holding conversations. Alternatively, they may also take a list of messages and return an output.

## Models, Prompts, and Chains

LangChain offers **prompt templates**, which are predefined recipes for generating prompts for language models. To combine chains, we can use **LCEL** to declaratively compose chains, or Python classes (legacy) such as LLM Chain and others. A **chain** is a sequence of these components (pipeline).

## Extending Chatbot by Adding Memory

The ability to store information about past information is called **memory**. In a chain, memory is interacted with twice, after user input but before chain execution (read from memory) and then after core logic but before output (write to memory).

## Extending Chatbot by Adding RAG and Vector Database

## Extending Chatbot by Adding RAG and Memory

## Chatbot Technical Architecture

## Deploy Chatbot to VM

## ChatBot Setup Guide

## Deploy Chatbot to OCI Data Science
