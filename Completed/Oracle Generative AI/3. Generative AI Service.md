The OCI Generative AI Service is a **fully managed service** that provides a set of customizable LLMs via one API to build generative AI applications. It supports a collection of high-performing pretrained models from Meta and Cohere, flexible fine-tuning, and dedicated AI clusters to host fine-tuning and inference workloads. It is build to understand, generate, and process human language at a massive scale, and may be used to generate text, summarize, extract data, classify, and hold conversations.

## Pretrained Foundational Models

First, we have **generation** models, namely, **command**, **command-light**, and **llama2**. These are intended to generate text from instructions. Next, we have **summarization** models including **command**. Finally, there are the **embedding** models (embed-multilingual, embed-english, and embed-english-light) which are used to create vector embeddings of text for semantic search and multilingual models.

## Fine-tuning

The OCI models are all able to be fine-tuned using custom data. By doing so, we improve the model performance on specific tasks and improve model efficiency. We use fine-tuning *when a pretrained model doesn't perform our task well or we want to teach it something new*. OCI Generative AI uses the **t-few fine-tuning** method to enable fast and efficient customizations, which is a parameter efficient fine-tuning method that updates only a portion of the model's weights.

## Dedicated AI Clusters

OCI provides RDMA-based GPU clusters for fine-tuning and inference workloads. It establishes a dedicated AI cluster that includes dedicated GPUs and an exclusive RDMA cluster network for connecting GPUs, and is visible as a single logical view. GPUs allocated for a generative AI task are isolated from other GPUs.

## Generation Models\

LLMs understand tokens instead of characters. Tokens may be parts of a word, an entire word, or punctuation. Simple text may be one token per word meanwhile complex text can be 2-3 tokens per word on average.

The command (52B) and command-light (6B) models are used for text generation, chat and text summarization. The llama2 (70B) model is highly performant, open-source, and optimized for dialogue use cases.

### Model Parameters
- **Temperature** is a hyperparameter that controls the randomness of the LLM output. As it increases, it flattens the token probability distribution to increase the chances of less likely tokens being chosen. A temperature of 0 makes the model deterministic (greedy).
- **Top K** tells the model to pick the next token from the top $k$ tokens in its list. It is essentially limiting the distribution to select only from those $k$ options. **Top P** does the same except it uses the sum of probabilities instead. **Stop sequences** are an arbitrary string that tell the model to stop generating more content.
- **Frequency** and **presence penalties** are used to reduce repetition in responses. As a token appears more in an output, its penalty increases by number of occurrences if using frequency penalties, and gets penalized uniformly if using presence penalty.
- **Show likelihoods** assigns a likelihood between -15 and 0 to all tokens, representing the likelihood of that token following the last token. I guess that it is used for debugging?

## Summarization Models
**Summarization models** take text and generate succinct versions of original text relaying its most important information.

### Parameters
- **Temperature** determines how creative the model should be. You may use 1 through 5 as the model's temperature.
- **Length** determines the approximate length of the summary, ranging from short, medium, and long.
- **Format** controls the writing format of the summary, such as paragraphs and bullet points.
- **Extractiveness** is how much of the input to reuse in the summary. High extractiveness leans towards reusing sentences verbatim meanwhile low extractiveness tends to paraphrase.
## Embedding Models
**Embedding models** create vector representations of input data that are consumable by decoder models and vector databases. We can measure the **semantic similarity** of two or more embeddings by computing its numeric similarity. To get **numeric similarity**, we may compute its cosine and dot product. Embeddings that are numerically similar are also semantically similar.

Vector databases are a main use case of embeddings because they enable RAG and semantic search.

## Prompts & Prompt Engineering
**Prompts** are inputs provided to an LLM in order to elicit a response. **Prompt engineering** iteratively refines prompts to produce better (in our opinion) outputs from an LLM. We use a few strategies to do this, such as in-context learning and few-shot prompting.

**In-context** learning conditions an LLM by including instructions and/or demonstrations of the task it is meant to complete. More specifically, **few-shot** prompting is when we specifically provide a few examples in our prompt to guide the model into getting what we want. We say **k-shot-prompting** when we provide **k** examples in our prompt.

LLMs are trained using specific prompt formats. They contain different sections that may be used for instructions, context, and user prompts. However, they will begin in plaintext that is converted into embeddings.

**Chain-of-thought** prompts the LLM to give better answers by having it break down its reasoning into individual steps. **Zero-shot chain-of-thought** is a similar method however we use key phrases to instruct the LLM to produce its steps on its own instead of us providing the examples. We can use phrases such as "Let's think step by step." to induce this. The main drawback to this is the model context length limit because our responses are becoming longer.

![[Pasted image 20240730144023.png]]

To fine-tune an LLM, we typically begin with a single prompt. We can try out few-shot prompting, but we may want to try out RAG to add context. Next, we'd try fine-tuning and return to RAG with the new model, and repeat until we have a good solution.

## Fine-Tuning and Inference in OCI

In OCI, we create a **custom model** by taking a pretrained model as a base and fine-tuning it with our own datasets.

1. First, we create a **dedicated AI cluster**.
2. We gather training data next.
3. Next, we start fine-tuning.
4. Finally, we have a fine-tuned model.
A **model endpoint** is a designated endpoint on a dedicated AI cluster where an LLM can accept and respond to user requests.
To create one, we:
1. Create a dedicated AI cluster.
2. Create and endpoint.
3. Serve model.
Dedicated AI clusters are effectively a single-tenant deployment where the GPUs in the cluster only host your own custom models. Because it isn't shared with anyone else, the model's throughput is consistent. The minimum cluster size is easier to estimate based on the expected throughput. We have two separate cluster types: **fine-tuning** for training a model and **hosting** to hosting a custom model endpoint for inference.

### Fine-tuning and Inference
**Vanilla** fine-tuning is expensive because it updates all of the model's weights. Instead, we like **t-few fine-tuning** because it selectively updates only a fraction of the model's weights. It uses few-shot parameter efficient fine tuning (PEFT), meaning that instead of updating the model's parameters, it only adds additional layers (typically about ~0.01% of the total model size). Only the t-few layer weights are updated throughout the process and saves a lot of training time, but makes the model somewhat slower overall.

Because GPU memory is limited, switching between models can incur significant overhead due to reloading the full GPU memory. However, if we use the same model with only slight weight variations, we can deploy multiple models on the same set of GPUs with minimal memory updates. This method is known as **parameter sharing**.

## Dedicated AI Cluster Sizing and Pricing

![[Pasted image 20240730165124.png]]In short, fine-tuning clusters require 2 units and hosting requires a minimum of 1 unit. Fine-tuning clusters can be used to fine-tune multiple models, and hosting clusters can host up to 50 different fine-tuned models if tuned using t-few fine tuning. Hosting clusters let you create up to 50 endpoints (one for each custom model) as well. However, hosting clusters have a minimum commitment of 744 hours (1 month) per cluster.

![[Pasted image 20240730165853.png]]

## Fine Tuning Configuration

When creating a fine-tuning cluster, we are presented with some advanced options.

![[Pasted image 20240730170238.png]]

We can understand our fine-tuning results using accuracy and loss. **Accuracy** is the measure of how many predictions are correct out of all of the predictions in a test set of data. **Loss** is the measure of how bad or wrong a prediction is. We need a loss function to compute this. As a general rule of thumb, loss should decrease as the model improves, and is often a measure to early stop when loss stops decreasing.

## Security

To ensure the security and privacy of customer workloads, GPUs allocated for tasks are isolated from other GPUs. Similarly, a dedicated GPU cluster only handles fine-tuned models for one customer. The customer's data access is restricted to only their own tenancy.

OCI also implements OCI Identity & Access Modules (IAM) to provide authentication and authorization. All customer data is encrypted in OCI buckets.