Goals of this module:

- Explain the fundamentals of OCI GenAI service
- Use pretrained foundational models for generation, summarization, and embedding
- Fine-tune base models with custom datasets
- Create and use model endpoints for inference
- Create dedicated AI clusters for fine-tuning and inference
- Explore OCI GenAI security architecture

## RAG

Retrieval augmented generation utilizes provided documents to extract necessary information and use it to produce a more informed answer. RAG models retrieve documents and pass them to a seq2seq model.

The **retriever** is responsible for sourcing relevant information from a large corpus or database. Next, the **ranker** evaluates and prioritizes the information retrieved by the retriever. Finally, the **generator** uses that ranked information along with the user's original query to create a nicely formatted answer.

For each input query in RAG sequence, the model retrieves a set of relevant documents or information. Then, it considers all these documents together to create a response that reflects the combined information.

Alternatively, a RAG token model considers pieces of the document for different parts of the output. The response is constructed incrementally, with each part reflecting the information from the documents retrieved for that specific part.

The RAG pipeline begins with the base documents. In the **ingestion** phase, We decide how we want to break the documents into chunks for ingestion. Typically, we can break them into paragraphs or chapters. Then, we convert the chunks into embeddings which are stored and indexed in a vector database.

Next, we enter the **retrieval** phase, which takes the query and uses its embedding to search for the most relevant chunks to return the top $k$ results. In the **generation** phase, the model takes the top $k$ results from before and uses it to generate a response for the user.

### Applications

The model can use a query and chat history to generate an **enhanced prompt**, which is then passed into an embedding model to perform similarity search. The data returned from the search can then be used to create an **augmented prompt**, which is finally fed into an LLM to create a highly accurate response.

![[Screenshot 2024-07-30 at 6.10.26 PM.png]]

## Vector Databases

LLMs without RAG rely on internal knowledge and its prompt only for information, so they will only have relevant information up until the time of training. If we add RAG, however, we can incorporate a vector database "memory" that we can continuously update without having to retrain our LLM.

![[Screenshot 2024-07-30 at 6.16.16 PM.png]]

When comparing two vectors, the **cosine distance**:

$$
1-\frac{A\cdot B}{||A||-||B||}
$$

gives us the angle distance between both vectors. A cosine distance of 0 means they face the same direction. The **dot product**:

$$
A\cdot B=\sum^n_{i=1}A_iB_i
$$

is the projection of A onto B. We use cosine distance because it relies less on the magnitude and more the angle difference between the two vectors. We can use K-nearest neighbors or ANN (approximated) to find similar items for clustering. For larger scale similarity searches, we tend to use ANN methods like HNSW, FAISS, and Annoy due to their efficiency. To add a vector to a vector database, we take our vector, index it, store it, and then we can retrieve it in querying and perform post-processing. Vector databases are highly accurate, highly optimized for low latency and parallel processing, and are highly scalable.

With regards to LLMs, vector databases are used to address hallucinations and inaccuracies, augment prompts with relevant information, and help avoid token limits by using the most relevant topics. It is cheaper than fine-tuning LLMs, provides real-time updated knowledge bases, and caches previous prompts and responses to reduce costs and improve performance.

## Keyword Search

Keyword search is the simplest form of search and operates by counting the exact matches of user-provided keywords in the database or index. It is cheap but may not always return the best results.

## Semantic Search
**Dense retrieval** relies on the embeddings of both queries and documents to identify and rank documents by returning the nearest neighbors. **Reranking** assigns a relevance score to each result, typically after an initial ranking has been created.