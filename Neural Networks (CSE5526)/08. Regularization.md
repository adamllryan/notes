## Regularization

The idea of regularization is to improve the generalization of our model. We already mentioned one method of regularization: early stopping.

Regularization adds the term $\alpha R(w)$ to the cost function, where $\alpha$ is the regularization parameter and $R(w)$ is the regularization term. The regularization term is a function of the weights. The regularization term is used to penalize large weights. As a whole, it looks like:

$$
E(n) = \frac{1}{2} \sum_{i=1}^{N} (d_k(n)-y_k(n)^2 + \alpha R(w)
$$

The most common regularization terms are:

1. **L1 Regularization**: $R(w) = \sum_{i=1}^{N} |w_i|$
2. **L2 Regularization**: $R(w) = \sum_{i=1}^{N} w_i^2$

L1 regularization is used to create a sparse network. It is used to reduce the number of weights in the network. L2 regularization is used to prevent the weights from becoming too large.

### Ridge Regression

Ridge regression is a form of L2 regularization. It is used to prevent the weights from becoming too large. The cost function for ridge regression is:

$$
R(w) = ||w||^2_2 = \sum w_i^2
$$

This is also known as Tikhanov regularization or weight decay.

## L1 Regularization

The goal of L1 regularization is to zero out any weights that don't contribute to the success of the network. L1 regularization uses $R(w)=||w||_1=\sum|w_i|$. L1 regularization also goes by lasso regression.

## L2 Regularization

The goal of L2 regularization is to prevent the weights from becoming too large.While having large weights is not necessarily bad, having some weights be too large can cause loss of test performance that isn't detectable during train. It helps prevent overfitting of the model.

L2 regularization uses $R(w)=||w||_2^2=\sum w_i^2$.

## Elastic Net

Elastic Net is a combination of L1 and L2 regularization. It uses the cost function:

$$
R(w) = \lambda_1||w||_2^2+\frac{1-r}{2}||w||_1
$$

where $\lambda_1$ is the L2 regularization parameter and $r$ is the L1 regularization parameter. Elastic Net is used to prevent overfitting and to create a sparse network.

## Dropout

Dropout is a technique used to prevent overfitting. It works by randomly setting a fraction of the input units to 0 at each update during training time. This helps prevent overfitting because the network cannot rely on any given input to be present. Dropout is a form of regularization. It is used to prevent overfitting and to create a sparse network.
