To estimate unknown parameters, we can use **linear least squares**, which aims to minimize the total square error in $E=\frac{1}{2}\sum^N_{p=1}{\epsilon^2_p}=\frac{1}{2}\sum^N_{p=1}(d_p-y_p)^2$. The derivation of this is not nice. For an m-dimensional input variable, we can define $w$ and $x_p$ which are our weight vector and p-th input vector, respectively. Now, our equation is $E=\sum^N_{p=1}(d_p-w^Tx_p)^2=\lVert d-Xw\rVert^2$. After derivations, we are left with $w=(X^TX)^{-1}X^Td$, also known as the normal equation. While easy to implement and fast, the inverse of the matrix is expensive to compute (we often use estimated inverse) and we need to use the batch approach, where all of our training data is used at once. It is not iterative, so it may be expensive to try to run all at once. 

# Gradient Descent

**Gradient Descent** is an iterative approach to finding optimal parameter values. We begin by randomly initializing our w weights, and then moving in the opposite direction of the derivative. A directional derivative lets us find the rate of change at any point, and to move towards the min, we just move in the other direction. 

