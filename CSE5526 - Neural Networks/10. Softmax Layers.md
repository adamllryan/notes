---
id: 10. Softmax Layers
aliases: []
tags: []
---

## Softmax Layer

When we want to classify exclusive data, softmax function is used to normalize the output of our networks to a probability distribution. We take the _activation potential_ of each output neuron and apply:

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
$$

Where $z$ is the vector of activation potentials, and $j$ is the index of the output neuron. The denominator is the sum of the exponentials of the activation potentials of all the output neurons. This ensures that the output of the softmax function is a probability distribution, i.e. the sum of all the output neurons is 1.

Because of the change in the output layer, we also have a change in loss function. We use the cross-entropy loss function, which is defined as:

$$
E(w) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
$$

Where $y$ is the true label, and $\hat{y}$ is the predicted label.

## Sequential Data

When we have sequential data, we have many options to handle it.

### Data Splicing

We take a sequence (sliding window) of data and use it as input to the network. In order to get an output, we can use the last element of the sequence, or we can use the average of the sequence. However, using data splicing introduces latency because we need a buffer of data to begin processing.

### Recurrent Neural Networks (RNN)

RNNs are designed to handle sequential data and have a "memory" of the previous outputs. At some point within the network, the output from one part of the network is stored and fed back in during the next cycle as input data at another layer (the loops do not need to be strictly input and output layers). Compared to feed-forward networks, RNNs are much more suited for variable-length inputs while feed-forward networks are only suited for strict-length inputs.
