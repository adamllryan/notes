---
id: 10. Softmax Layers
aliases:
  - softmax
tags:
---

## Softmax Layer

When we want to classify non-exclusive data, we apply a softmax function to normalize the output of our networks to a probability distribution. We take the _activation potential_ of each output neuron and apply:

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
$$
where $\sigma(z)_j$ is our resulting probability at index $j$, $z_j$ is our input at index $j$, and the denominator is the sum of all $e^{z_j}$, or every input $z_j$ as an exponent of $e$, summed. 

We can think of the activation potential of every neuron as being the *estimated unnormalized log probability* for it's class. If we sum all outputs, we get 1, and each output (post-softmax) will be the probability of that classification being true. 

Because of the change in the output layer, we also have a change in loss function. We use the cross-entropy loss function, which is defined as:

$$
E(w) = -\sum_{i=1}^{C} d_i \log(y_i)
$$

Where $d$ is the true label, and $y$ is the predicted label.

## Sequential Data

When we have sequential data, we have many options to handle it. In this scenario, we are dealing with data that may be dependent on time and the prior data before. We cover two options: data splicing and RNNs. 

### Data Splicing

**Data splicing** is the technique of taking a *window* of data across time $t$ and aggregating them such that our new data is the same shape as one element, or **frame**, of our window. 

Splicing **features** (inputs) is straightforward. We perform feature splicing by choosing a current context as well as adjacent feature frames, and passing them into the network. The main change in the network now is that for a window of size $n$, the network must have $n$ times as many inputs to handle each piece of data concurrently. The issue with this is that the shape of the network depends on how large our window is. 

Splicing **labels** and works differently. Our output **labels** will show up as another window, but how do we aggregate them? We can do so by either averaging the frames together or selecting the final frame. 

Using data splicing introduces problems in the network, such as:
- Increased units overall. 
- Has one more hyperparameter (window size). 
- Lowered computational efficiency. 
- Increased latency. 
- No learning from correlations. 
- Requires a buffer (at least the size of window) to begin processing. 

### Recurrent Neural Networks (RNN)

RNNs are designed to handle sequential data and have a "memory" of the previous outputs. At some point within the network, the output from one part of the network is stored and fed back in during the next cycle as input data at another layer (the loops do not need to be strictly input and output layers). **Loops** (or cycles) are feedback neurons or layers that pass information from a previous cycle back into the network for the next.  Compared to feed-forward networks, RNNs are much more suited (ideal, even) for variable-length inputs while feed-forward networks are only suited for strict-length inputs.

Dealing with recurrent networks, we may simplify our diagrams of our networks by shrinking describing each layer as one neuron and unfolding recurrent connections along time. 

![[Folded RNN.png]]
![[Unfolded RNN.png]]