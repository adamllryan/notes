## Machine Translation

**Machine Translation** is the idea of using algorithms to convert languages to another in text form.

The first approach to machine text translation was **rule-based machine translation**, which processes text based on a set of linguistic rules and dictionaries developed by human language experts. However, languages do not follow a defined set of rules very well.

The second approach is [[09. Machine Translation#Statistical Machine Translation (SMT)|statistical machine translation]], which relies on probabilistic models trained using supervised machine learning.

The modern approach is to use [[09. Machine Translation#Neural Machine Translation (NMT)|neural machine translation]], or machine translation using something like a recurrent neural network to solve a sequence-to-sequence problem.

## Encoder-Decoder Approach

The common approach to seq2seq applications is to use an encoder and decoder, which are two separate neural networks that are trained to work in a pipeline.

The **encoder**'s job is to take in a sequence of text (or whatever input we want) and recursively encode each piece of text, returning its final [[12. Recurrent Neural Networks#LSTM|hidden state]] vector $h_i$ representing a numerical meaning of the original input.

Then, it passes that encoded state to the **decoder**, which takes the last state $h_i$ as input and decodes it back into an equivalent text representation.

Taken from lecture:

The **decoder**'s states are initialized from the encoder's last state ($h_i$), and the decoder's output at time $i$ becomes its input at time $i+1$. The number of states in the decoder may depend on the length of the output sequence.

### Early Implementation

In 2014, Google released a research publication on Sequence to Sequence Learning. In that paper, Google researchers developed a machine translation model that translates English to French using LSTM neural networks as an encoder and decoder. Each LSTM network had 4 LSTM layers (100 LSTM cells each) and a softmax layer pooling at the output layer. The researchers utilized stochastic gradient descent without momentum, batches of 128 sequences, and a manually adjusted learning rate after 5 epochs.

This model did have limitations, though. The final hidden state of the encoder was a single vector, which had to be able to represent a *whole* sentence. This means one vector must be able to represent a sentence of variable length, regardless of how long it may be. This leads to information loss as the input sentence increases in length.

Additionally, the model had trouble ensuring that important information at the start of the sequence (such as the subject of the sentence) was retained as the sentence was parsed as well as keeping focus on parts of the source sentence most important at different times.

## Attention

Our solution to seq2seq attention loss in our framework is (obviously) to implement an **attention mechanism**.

This newly added attention mechanism enables the model to:

- Align and translate jointly between the source and target languages.
- Soft-search for a set of positions in the source sentence that are most relevant to each point of the target language in the decoder.
- Encode the input sequence into a sequence of vectors rather than a single vector.

Here is the updated process:

First, the encoder will export all of its hidden states instead of only the final state. The sequence of hidden states is passed through our attention mechanism (a deep feed-forward network) which scores each hidden state and then softmaxes it to normalize into "attention score" weights. We then apply those weights to the hidden states and sum them together to produce a context vector $c_i$. 

The decoder, at that same sequence, will receive context $c_i$ along with its previous output $y_{i-1}$ to produce $y_i$. The decoder passes state $s_i$ back to the attention mechanism DNN as its new input, which re-scores the encoder's hidden states using this new state context. The attention DNN uses this state to produce the next $c_i$, and this repeats until an `<eos>` token is produced or forced. 

