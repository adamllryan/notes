A **Boltzmann machine** is a stochastic learning machine consisting of visible and hidden units linked by symmetric (two-way) connections. They use **stochastic** neurons with sigmoid activation functions where the state of the neuron is:

$$
x_i=
\begin{cases}
1 & \text{ if probability } \phi(v_i)\\
-1 & \text{ if probability } 1-\phi(v_i)
\end{cases}\\
\text{ where }
 \phi(v)=\frac{1}{1+e^{-\frac{2v}{T}}}
 $$

The goal of Boltzmann learning is to produce a network that can model the probability distribution of input patterns (visible variables). It should be able to compute the probability of the next observation, learn the model's parameters from data, and estimate likely values given partial observations. It is very similar to **statistical machines**. 

This machine uses **energies** to produce its outputs. Given energy at state $i$ as $E_i$, probability $p_i$, and temperature $T$, we get:

$$
p_i=\frac{1}{Z}e^{-\frac{E_i}{T}}\text{ where } Z=\sum_ie^{-\frac{E_i}{T}}
$$
$Z$ is our partition function. 

We observe that lower energy states have a higher probability of occurring. As T decreases, the probability is concentrated on a smaller subset of low-energy states. 