A **Boltzmann machine** is a stochastic learning machine consisting of visible and hidden units linked by symmetric (two-way) connections. They use **stochastic** neurons with sigmoid activation functions where the state of the neuron is:

$$
x_i=
\begin{cases}
1 & \text{ if probability } \phi(v_i)\\
-1 & \text{ if probability } 1-\phi(v_i)
\end{cases}\\
\text{ where }
 \phi(v)=\frac{1}{1+e^{-\frac{2v}{T}}}
 
$$

The goal of Boltzmann learning is to produce a network that can model the probability distribution of input patterns (visible variables). It should be able to compute the probability of the next observation, learn the model's parameters from data, and estimate likely values given partial observations. It is very similar to **statistical machines**.

This machine uses **energies** to produce its outputs. Given energy at state $i$ as $E_i$, probability $p_i$, and temperature $T$, we get:

$$
p_i=\frac{1}{Z}e^{-\frac{E_i}{T}}\text{ where } Z=\sum_ie^{-\frac{E_i}{T}}
$$

$Z$ is our partition function.

We observe that lower energy states have a higher probability of occurring. As T decreases, the probability is concentrated on a smaller subset of low-energy states.

To compute the probability of a new observation, we start by dividing the network into $x_\alpha$ visible units and $x_b$ hidden units. Given parameters $w$ (weights), we compute $P(x_\alpha)$ with:

$$
P(x_\alpha)=\sum_{x_\beta}p(x)=\sum_{x_\beta}\frac{1}{Z}e^{-\frac{E(x)}{T}}=\sum_{x_\beta}\frac{1}{Z}e^{\frac{1}{Z}e^{-\frac{E(x)}{T}}}=\sum_{x_\beta}\frac{1}{Z}e^{\frac{1}{Z}x^Twx}
$$

Boltzmann machines learn using the log-likelihood of data. We attempt to maximize the likelihood of the visible units that take training patterns. Assuming each pattern is independent, the log probability of the training sample is:

$$
L(w)=\sum_{x_\alpha}[\log\sum_{x_\beta}e^{-\frac{E(x)}{T}}-\log\sum_xe^{-\frac{E(x)}{T}}]
$$

Therefore, the gradient of $L(w)$ is

$$
\frac{\partial L(w)}{\partial w_{ji}}=\frac{1}{T}a(p^+_{ji}-p^-_{ji})
$$

where $p^+_{ji}=\sum_{x_\alpha}\sum_{x_\beta}P(x_\beta|x_\alpha)x_jx_i=\sum_{x_\alpha}E_{x_\beta|x_\alpha}{x_jx_i}$ and $p^i_{ji}=K\sum_xp(x)x_jx_i\approx E_x{x_jx_i}=<x_jx_i>$ and $K$ is the number of visible neurons.

To maximize $L(w)$, use gradient descent. The learning phase has two parts:

- Positive phase, where the network operates in the clamped condition (visible neurons take on training patterns).
- Negative phase, where the network operates freely without the influence of external input.

The high-level idea is to learn the distribution and the features of that distribution.

## Remarks

## Motivation

## Vanishing Gradient

**Unsupervised pre-training** is an alternative way to initialize deep neural networks. We build the training up from the bottom where we train a shallow model (low number of layers) and then freeze their weights. Then, we attach a new layer to the end of the network and train only those layer weights, and so on.

A **restricted boltzmann machine** is boltzmann machine with one visible layer and one hidden layer. It has no connection within each layer.