When handling data that varies over time or is sequential, traditional neural networks are not sufficient. DNNs are not able to handle data that has dependencies on previous information. 
### Data Splicing
**Data splicing** is the technique of taking a *window* of data across time $t$ and aggregating them such that our new data is the same shape as one element, or **frame**, of our window. 

Splicing **features** (inputs) is straightforward. We perform feature splicing by choosing a current context as well as adjacent feature frames and passing them into the network. The main change in the network now is that for a window of size $n$, the network must have $n$ times as many inputs to handle each piece of data concurrently. The issue with this is that the shape of the network depends on how large our window is. 

Splicing **labels** and works differently. Our output **labels** will show up as another window, but how do we aggregate them? We can do so by either averaging the frames together or selecting the final frame. 

Using data splicing introduces problems in the network, such as:
- Increased units overall. 
- Has one more hyperparameter (window size). 
- Lowered computational efficiency. 
- Increased latency. 
- No learning from correlations. 
- Requires a buffer (at least the size of the window) to begin processing. 

### Recurrent Neural Networks (RNN)

RNNs are designed to handle sequential data and have a "memory" of the previous outputs. At some point within the network, the output from one part of the network is stored and fed back in during the next cycle as input data at another layer (the loops do not need to be strictly input and output layers). **Loops** (or cycles) are feedback neurons or layers that pass information from a previous cycle back into the network for the next.  Compared to feed-forward networks, RNNs are much more suited (ideal, even) for variable-length inputs while feed-forward networks are only suited for strict-length inputs.

Dealing with recurrent networks, we may simplify the diagrams of our networks by shrinking and describing each layer as one neuron and unfolding recurrent connections over time. 

![[Folded RNN.png]]
![[Unfolded RNN.png]]