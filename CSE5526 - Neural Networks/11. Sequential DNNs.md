When handling data that varies over time or is sequential, traditional neural networks are not sufficient. DNNs are not able to handle data that has dependencies on previous information. 
### Data Splicing
**Data splicing** is the technique of taking a *window* of data across time $t$ and aggregating them such that our new data is the same shape as one element, or **frame**, of our window. 

Splicing **features** (inputs) is straightforward. We perform feature splicing by choosing a current context as well as adjacent feature frames and passing them into the network. The main change in the network now is that for a window of size $n$, the network must have $n$ times as many inputs to handle each piece of data concurrently. The issue with this is that the shape of the network depends on how large our window is. 

Splicing **labels** and works differently. Our output **labels** will show up as another window, but how do we aggregate them? We can do so by either averaging the frames together or selecting the final frame. 

Using data splicing introduces problems in the network, such as:
- Increased units overall. 
- Has one more hyperparameter (window size). 
- Lowered computational efficiency. 
- Increased latency. 
- No learning from correlations. 
- Requires a buffer (at least the size of the window) to begin processing. 

More commonly, however, we use [[12. Recurrent Neural Networks|recurrent neural networks]] to solve sequential data problems, covered in the next set. 