## Gradient Descent

**Gradient Descent** is an iterative approach to finding optimal parameter values. We begin by randomly initializing our w weights, and then moving in the opposite direction of the derivative. A directional derivative lets us find the rate of change at any point, and to move towards the min, we just move in the other direction.

## Least Mean Squares

For a one-dimensional $x$, our weight update formula for a neuron using Gradient Descent is $w(n+1)=w(n)-\eta\nabla_wE(n)=w(n)+n[d_p(n)-y_p(n)]x(n)$. This is the Least-Mean-Squares (LMS) algorithm. This is the same math as for the perceptron learning rule. With this, we run into the problem of getting stuck in local minima and plateaus.

## Stochastic Gradient Descent

**Stochastic Gradient Descent** only uses one single feature instance to compute the gradient. This is **online learning**. We iterate over each example (each iteration with separate updates) which is known as an **epoch**. This method is less stable but is much, much cheaper. It also helps us escape local minima! The solution to instability is called **annealing**, which will be discussed below.

## Batch Gradient Descent

**Batch Gradient Descent** is the same idea as above, except we use the whole dataset for each iteration. In this case, simple conditions guarantee convergence to a local minimum. The formula for this is $w(n+1)=w(n)-\eta\nabla_wE$ where

$$
\begin{align}\nabla_wE=\frac{1}{N}\sum^N_p\\=1(d_p-w^Tx_p)x_{p,1}\\=\frac{1}{N}(d-Xw)^TX\end{align}
$$

. In this, we take the gradient of every point each time and then return the mean.

## Mini-batch Gradient Descent

**Minibatch Gradient Descent** is the combination of both **batch** and **stochastic** gradient descent. The difference with this implementation is that in each iteration, small sets of data examples are used for the gradient instead of the whole dataset or one sample. Minibatch may still get stuck at local minima and not converge to the optimal solution, but it is less erratic than stochastic.

The approach always depends on the problem, dataset, and resources.

The professor claims everyone uses mini-batches, which seems reasonable.

## LMS for Nonlinear Neurons

What if we have nonlinear (but differentiable) activation functions? We use the same update form, but we change the gradient based on the chain rule.

$$
\begin{align}\frac{\partial E}{\partial w_i}=\frac{\partial E\partial y\partial v}{\partial y \partial v\partial w_i}\\  
=[d(n)-y(n)]\phi'(v(n))x_i(n)\\  
=-e(n)\phi'(v(n))x_i(n)\end{align}
$$

$$
w_i(n+1)=w_i(n)+\eta\delta(n)x_i(n)$$  
For example, take a logistic sigmoid activation function. It is denoted as $\phi(v)=\frac{1}{1+\text{exp}(-av)}$. In this case, $\phi'(v)=a\phi(v)[1-\phi(v)]$.

$\eta$ is our learning rate. The value of $\eta$ is *very* important, because if it is too small, it takes longer to converge on a solution, and if it is too large, we may entirely miss the solution. A too-large $\eta$ can also cause a ping-pong effect, where it repeatedly skips the solution.

What we can do, as a solution, is to let $\eta$ be variable. The first approach is to use **stochastic approximation**, where we start with a large learning rate and decrease it after each iteration: $\eta(n)=\frac{c}{n}$ where $c$ is a positive parameter.  
We can also use **search-then-converge**, which uses the function $\eta(n)=\frac{\eta_0}{1+(n/\tau)}$ where $\eta_0$ and $\tau$ are positive. With this, the learning rate is about constant when $n$ is small compared to $\tau$, then as $n$ increase, the learning rate closely follows stochastic approximation.  
A third approach is **predetermined piecewise constant learning rate (PPCLR)**, which uses a constant $\eta$ within an epoch, but changes once a new epoch begins. This is a great approach, but we need to know what the appropriate values are and when/how to tune them.

## Momentum

Gradient Descent is slow during training, so what we can do is create a **momentum vector** $m$ and use it instead of the direct gradient to update weights. The function is defined as $m(n)=\beta m(n-1)-\eta\nabla_w E(n)$ and our weight function is $w(n+1)=w(n)+m(n)$. It can smooth oscillating situations and is essentially the weighted sum of all gradients.
