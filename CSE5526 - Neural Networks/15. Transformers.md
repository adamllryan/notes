When performing squential machine learning, it's logical to emphasize certain parts of the input data while putting less focus on the rest. [[13. Attention|Attention]] is the mechanism we use for this, but unless we make some improvements, we only have a limited amount of it. Encoder-decoder architectures are based on RNNs and are serialized, meaning they can't fully take advantage of modern hardware due to them being optimized for parallel compute.

## General Attention

The **general attention mechanism** takes in an $N\text{x}D$ **input** and $M\text{x}D$ **query** vector and produce an $M\text{x}D$ output vector. $D$ represents the dimensionality of each vector and $N$/$M$ represent the number of vectors we are dealing with.

First, each input $x$ is multiplied by both a learned weight matrix $W_k$ and $W_v$, which converts the inputs into a $k$ key and $v$ value pair. The dimensionality for each of these are the same, where $D_q=D_k=D_v$. The intuition behind using key and value pairs is to add more expressivity to the model.

Afterward, the query vector and key vectors are inserted into an **alignment** mechanism, which passes the values through $e_{ij}=\frac{q_j\cdot k_i}{\sqrt{D}}$, converting our values into simple scaled dot products. The results are then passed through a softmax layer, producing an attention map where the sum of the weights add to one.

The attention map vectors are then converted to our final values when they are passed into the function $y_j=\sum_i{a_{ij} \cdot v_i}$ alongside our value vectors from earlier.

![[General Attention.png]]

## Self-Attention

The **self-attention mechanism** operates differently from the general mechanism because instead of using a query and input vector, the self-attention mechanism calculates the query vectors from the input vectors themselves. The query matrix is calculated by $q=xW_q$ where $W_q$ is a learned weight matrix.

![[Self-attention.png]]

## Multi-head Attention

**Multi-head attention** takes multiple self-attention blocks and parallelizes them to give a network the ability to learn multiple different points of attention. Each self-attention block is referred to as a **head**. Each output of each head is concatenated or added together afterwards.

![[Multihead Attention.png]]

## Masked Self-Attention

In order to prevent unintended interactions, mainly query vectors interacting with future key vectors, we use **masking**. Future combinations are set to $-\infty$ to ensure that they do not continue through the attention mechanism.

![[Masked Attention.png]]

## Positional Encoding

So far, this setup will not get us much. The order of our inputs does not change our model outputs, meaning that our self-attention mechanism is permutation-invariant. Normally, the position of input vectors is important to getting our desired results, so this needs to be fixed. Our solution to this is **positional encoding**, where we create an additional position-based encoding vector and concatenate it into our regular input vectors before feeding them into the self-attention blocks.

These positional encodings must follow a set of rules in order for them to work correctly:

- They should output a unique encoding at each time step.
- The distance between any two time steps should be consistent.
- The method should be generalizable to longer sequences.
- The method should be deterministic (repeatable).

Some such options that meet these criteria are to learn a look-up table and to use a fixed function, such as a sinusoid.

## Transformers

**Transformers** are one of the latest forms of machine learning models used for sequence modelling problems. They have a large memory requirement to use but they are expressive and highly parallelizable, capable of learning long sequences.

They are built upon the idea of encoders and decoders. The **encoder** segment takes an input, applies a positional encoding, then passes it through a multi-head self-attention block. These outputs are aggregated with passthrough data that skips the attention block. Afterward, the data is pushed through a layer norm block to normalize the data, and then is passed into a traditional multilayer perceptron network. This MLP also uses a passthrough connection, aggregated with the MLP's outputs. Finally, the data is put through another layer norm block before we receive our final outputs.

![[Transformer Encoder.png]]

The transformer **decoder** takes this input and begins similarly with a positional encoder block. Instead of a regular multi-head block, the decoder uses the **masked** version so that future information is not used before necessary. Passthrough is used again, being aggregated then normalized. The decoder adds a section between the MLP block and masked self-attention block, this time passing the data through a multi-head attention block using the context vector for the key-value pair inputs and the previous layer's output as query input. It once again aggregates passthrough and then continues like the encoder.

![[Transformer Decoder.png]]
