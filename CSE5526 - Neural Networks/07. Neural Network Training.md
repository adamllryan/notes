# Supervised Learning

We divide our datasets into train, test, and validation sets. Typically, 70 or 80 percent of our data is split into our training set and the rest is divided into test and validation sets. Every few epochs, we check the error on the validation set and stop training when the validation error begins to increase.

We can use trials as a way to not miss class imbalances in our data. We divide our sets of data into equal sizes (we could end up something like 4 test sets and 1 validation set) so that we can shuffle the order and train in multiple trials.

An **epoch** is defined as the process of iterating over all training inputs where each input separately updates the estimate. We use forward-pass for each item in the training set and accumulate the error. Then we use back-propagation to update the weights in the network.

## Learning Curves

A learning curve is a plot of the total error or validation performance after each epoch. It could be something like _mean-square error against number of epochs_ or _accuracy vs number of epochs_.

## Early Stopping

Early stopping is a technique used to avoid overfitting. We stop training when the validation error begins to increase. We can also use a patience parameter to stop training after a certain number of epochs without improvement. Otherwise, we will stop after a certain number of epochs.

## Cross Validation

Cross validation is a technique used to avoid overfitting. We divide our data into k-folds and train k times, each time using a different fold as the validation set and the rest as the training set. We then average the results to get the final model.

# Decision Designs

There are 3 main components to making a design decision: architecture, activation function, and learning rule. Architecture is comprised of connections, layers, units, type of connections, and initialization values. The learning rule encapsulates the algorithm, optimization approach, stopping criteria, and the learning rate.

## Network Structure

We take what our input and output data is and then decide how many input/output neurons we need. For an image, the number of input neurons we need is the number of pixels of the input image. In general, the input layer is controlled by the dimensionality of the input data. The output layer is determined by your expected results.

The number of hidden layers and the number of neurons in each layer is determined by the complexity of the problem. We can use the rule of thumb of having the number of neurons in the hidden layer be between the number of neurons in the input and output layers. We may also get our number through trial and error, where we train and increase the number of layers until we have an error we like.

## Activation Function

We typically use sign/step for classification problems. For other problems, we mostly use Rectified Linear Unit (ReLU) or Sigmoid. ReLU is used for hidden layers and Sigmoid is used for the output layer. Sigmoid was much more common in the past, and ReLU is much more common.

# Vanishing Gradient Problem

The **vanishing gradient problem** is the issue where multi-layer networks fail to update earlier hidden layers (closer to input) as the amount of layers increase. The weights will become smaller with more layers, the closer we get to the input layer using back-propagation.

The vanishing gradient problem is caused by the use of the sigmoid activation function. The sigmoid function has a derivative that is always less than 1, and as we multiply these derivatives together, the product becomes smaller and smaller. This means that the weights in the earlier layers are updated less and less.

# Network Weight Initialization

Based on the number of input and output layers, we can initialize the weights in our network. We can use the following methods:

1. **Random Initialization**: We can initialize the weights randomly. This is the most common method.
2. **Xavier Initialization**: We can initialize the weights using a normal distribution with a mean of 0 and a standard deviation of $\frac{1}{\sqrt{n}}$, where n is the average number of neurons between the input and next layer. We can also perform a normal distribution between +/-r where $r=\sqrt{\frac{3}{n}}$.
3. **He Initialization**: We can initialize the weights using a normal distribution with a mean of 0 and a standard deviation of sqrt(2/n), where n is the average number of neurons between the input and next layer.

Using Xavier initialization will improve the distribution for each layer and will help with the vanishing gradient problem.

# Reducing Cost

## Batch Normalization

Proper initialization and activation does not guarantee success. The vanishing gradient is still a problem because the distributions between inputs and outputs change during training.

Batch normalization is a technique used to normalize the inputs to each layer. We calculate the mean and standard deviation of the inputs to each layer and normalize the inputs to have a mean of 0 and a standard deviation of 1. Batch normalization zero-centers and normalizes the inputs of each layer before the activation is applied.

NOTE: Batch normalization (in this case) is actually mini-match normalization.

Batch normalization is the process of:

1. Computing the sample mean and variance of the layer's input for a mini-batch.
2. Subtracting the mean from each sample in the batch, then dividing by the standard deviation.
3. Scaling and shifting the normalized inputs using two new parameters (gamma and beta).
4. During testing, using the mean and variance from the training set to normalize the testing data.

## Layer Normalization

Layer normalization is similar to batch normalization, but instead of normalizing the inputs to each layer, we normalize the inputs to each neuron. We calculate the mean and standard deviation of the inputs to each neuron and normalize the inputs to have a mean of 0 and a standard deviation of 1.

Layer normalization is the process of:

1. Computing the sample mean and variance of the layer's input for a mini-batch.
2. Subtracting the mean from each sample in the batch, then dividing by the standard deviation.
3. Scaling and shifting the normalized inputs using two new parameters (gamma and beta).
4. During testing, using the mean and variance from the training set to normalize the testing data.

# Fast Optimizers

## Nesterov Accelerated Gradient (NAG)

**Nesterov Accelerated Gradient** looks slightly ahead of the current position to get a value for our cost function. We use the equation:

$$
m = \beta m - \alpha \nabla_w J(\theta + \beta m)
$$

where $\beta$ is the momentum, $\alpha$ is the learning rate, and $\nabla_w J(\theta + \beta m)$ is the gradient of the cost function at the point $\theta + \beta m$. Nesterov Accelerated Gradient is, in general, faster than using regular momentum.

## Adaptive Gradient (AdaGrad)

**Adaptive Gradient** is a method that adapts the learning rate for each parameter. AdaGrad moves down the _flattest_ dimension instead of the steepest by pointing at the global optimum. Learning weights decay in an adaptive manner, and as a result, we may never reach the global optimum. WE use:

$$
s = s + \nabla_w J(\theta) \odot \nabla_w J(\theta)
$$

$$
w = w - \eta \nabla_w J(\theta) / \sqrt{s + \epsilon}
$$

where $\odot$ is the element-wise multiplication, $\alpha$ is the learning rate, and $\epsilon$ is a small value to avoid division by zero.

## Root Mean Square Propagation (RMSProp)

RMSProp fixes the problem of AdaGrad by using an exponentially weighted moving average. We still want the square of the gradient, however we use a decay rate $\beta$.

$$
s = \beta s + (1 - \beta) \nabla_w J(\theta) \odot \nabla_w J(\theta)
$$

where $\odot$ is the element-wise multiplication, $\alpha$ is the learning rate, and $\epsilon$ is a small value to avoid division by zero.

## Adaptive Moment Estimation (Adam)

**Adaptive Moment Estimation** is a combination of NAG and RMSProp. $\beta_1$ and $\beta_2$ are the momentum and scaling params, and it is often considered the bets optimizer, however this is not always true.

$$
m = \beta_1 m + (1 - \beta_1) \nabla_w J(\theta)
$$

$$
s = \beta_2 s + (1 - \beta_2) \nabla_w J(\theta) \odot \nabla_w J(\theta)
$$

$$
\hat{m} = \frac{m}{1 - \beta_1^t}
$$

$$
\hat{s} = \frac{s}{1 - \beta_2^t}
$$

$$
w = w - \eta \hat{m} \oslash \sqrt{\hat{s} + \epsilon}
$$

where $\odot$ is the element-wise multiplication, $\oslash$ is the element-wise division, $\alpha$ is the learning rate, and $\epsilon$ is a small value to avoid division by zero.
