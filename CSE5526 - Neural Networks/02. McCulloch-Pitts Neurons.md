## Artificial Neuron

An artificial neuron has a few functions we may use:

An **adder**, or weighted sum, or linear combiner, is a function that sums the weighted combination of our input signals.  
$u_k=\sum^m_{j=1}{w_{kj}x_j}$

**Activation potential** is the sum of the adder and bias, or $v_k=u_k+b_k$. We use **bias** as a "zero-th" input to shift the results of our activation function, or essentially use it as a way to control when our neuron fires. We can introduce bias after summing, or we can even treat it as its own fixed-weight input.

Our output is the result of passing $v_k$ into our activation function $\phi$, where $y_k=\phi(v_k)$.

We can easily represent NNs with vectors, where we have an input vector and a weight vector.

## McCulloch-Pitts Neuron Model

**McCulloch-Pitts** networks, developed in 1943, were a first-class of abstract computing machines: finite-state automata. These can compute any logic boolean function. They take bipolar (or binary) input and generate bipolar outputs.

### Emulating Logic Gates

We can emulate logic gates with MP neurons.

For negation, we can use weight -1 and bias 0.

For OR, we can use the weights 1 and bias 0. This works like below, except now any `True` will be `at least 0`.

For AND, we can set the weights as 1 and the bias as -1. This will give us the output -2 and -1 for all false values and then a 1 for the true values. This means our function can be `True if output >= 0 else False`.

## Artificial Neural Networks

We use two architectures of artificial networks: feed-forward and recurrent networks.

**Feed Forward** networks, when viewed as a graph, have no loops between nodes. **Recurrent** networks allow for loops in their graphs, meaning we can have our model pass its own values back into itself.

When we describe a neural network, we classify them by the number of nodes in each layer and the number of layers it has. A layer is one "row" or "column" in the model, where each layer is activated sequentially, and is connected to the layer before and after. A 10-4-2 network is a network with 10 inputs, 4 hidden nodes, and 2 output nodes. We consider it a 2 layer network, though, because we don't consider input nodes as a layer.

MP Neurons are limited to only bipolar inputs and outputs and we can't use it for learning because we have to guess the weights ourselves.
