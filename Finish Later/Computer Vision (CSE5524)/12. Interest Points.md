**Interest points** are regions in images that we can use to increase robustness against occlusions (barriers), improve articulation, and detect same-category variations. Instead of using template matches, we can use local matches, which describe points within local regions, or *local features*. 

We can use interest points to stitch two images together by finding matching interest points and transforming them about those points. To do so, we need to: 

1. Find a set of distinctive key-points (interest points). 
2. Define a region around each keypoint. 
3. Extract and normalize the region content. 
4. Compute a local descriptor from the normalized region. 
5. Match local descriptors. 

To find the same points in two images, we need a *repeatable* detector. We also need to be able to identify corresponding points using an algorithm that is geometric and photometric invariant. 

As a whole, we need the following requirements:

- A **repeatable algorithm** to produce same results between different images. 
- A **local** algorithm to handle occlusion and clutter. 
- A **large quantity** of regions to cover the object. 
- A **highly distinctive** set of regions (interesting). 
- An **efficient** algorithm for realtime performance. 

Some such detectors that meet these criteria are the FAST, Harris, and Shi-Tomasi algorithms. 

## Detectors
The **Features from Accelerated Segment Test** (FAST) detector compares the local neighborhood around each pixel to determine if the pixel is a good feature point. For each pixel $x$, we take the neighbors in a circle of radius $r$ and check if they are greater or less than $I(x)+T$ (intensity and threshold). We find the maximum length string of pixels that are all either greater or less than our condition and consider our pixel a feature if the length of that string is more than a certain hyperparameter of our choice (with radius $r=3$, the original paper used $n=12$ but we use $n=9$).  This effectively finds corner points in our image. 

The **Harris** detector is a more computationally expersive detector, which creates a 2x2 correlation matrix $M$. It computes:

$$
M=\sum_{x,y}{w(x,y)
\begin{bmatrix}
I_x^2 & I_xI_y \\
I_xI_y & I_y^2 \\
\end{bmatrix}
}
$$

where $w(x,y)$ is our weighting function and and $I$ is our gradient with respect to $x$ or $y$. We have a few options for our weighting functions. We can use a uniform window, which leaves us with:

$$
M=\sum_{x,y}{
\begin{bmatrix}
I_x^2 & I_xI_y \\
I_xI_y & I_y^2 \\
\end{bmatrix}
}
$$
Or, we can use a Gaussian window instead, which gives us:

$$
M=g(\sigma)
\begin{bmatrix}
I_x^2 & I_xI_y \\
I_xI_y & I_y^2 \\
\end{bmatrix}
$$

The harris detector is able to pick out flat, edge, and corner regions. After calculating $M$, we compute the cornerness function $R$ for the image, which is:

$$
R=\text{det}(M)-\alpha\text{trace}(M)^2=\lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2
$$

This is expensive, however. We can use a fast approximation algorithm to avoid computing eigenvalues by approximating $R$ to be:

$$
R=\text{det}[M(\sigma_I,\sigma_D)]-\alpha\text{trace}(M(\sigma_I,\sigma_D))^2=g(I^2_x)g(I^2_y)-g(I_xI_y)^2-\alpha [g(I_x^2)+g(I_y^2)]^2
$$

This gives us our corner points/interest points. 

Finally, we can use the **Shi-Tomasi** corner detector, which operates similarly to the Harris detector, but instead, we use:

$$
R'=min(\lambda_1,\lambda_2)
$$

Both of these detectors remain rotation invariant but not scale invariant. The naive approach is to conduct an exhaustive search to treat this. However, the best approach would be to design a function that is maximal at the correct region size for a location. 

Automatic scale selection chooses the best region size for our interest point detectors. We can use the scale-normalized **Laplacian of Gaussian** or blob/spot detector to handle our scale, defined as:

$$
L=\sigma^2 \cdot (G_{xx}(x,y;\sigma)+G_{yy}(x,y;\sigma))
$$

Using this, our local maxima in "scale space" are our points of interest. We use keypoint localization to detect maxima in scale space and then reject points with low image contrast using thresholding. Similarly, we can approximate our LoG by using **Difference of Gaussians** (DoG), which is cheaper because we no longer need second derivatives. 

A DoG Octave is a set of images for which smoothing $\sigma$ is doubled. 

**Local Descriptors** are invariant and distinctive. The simplest descriptor is a rasterized list of intensities within a patch. **Feature Descriptors** have the disadvantage of being greatly affected by small deformations. We can overcome this by creating a "histogram of gradient directions". Rotation invariant descriptors are found by finding the local gradient directions in a patch, finding the dominant direction of the overall gradient, and then rotating the patch according to this angle. 

**Scale invariant feature transform** (SIFT) divides the patch into subpatches, computes the gaussian weighted histogram of gradient directions for all pixels, and rasterizes and normalizes it. SIFT is fairly a fairly robust matching technique. It can handle significant changes in viewpoint and illumination and is fast and efficient (real-time). It also has variations such as SURF and has GPU-based implementations. 
