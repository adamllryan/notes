**Supervised learning** takes training data with a set of corresponding class labels and builds a classifier to predict output labels based on unseen test data. It needs to infer a function that separates the data into desirable classes.

The learning process begins with splitting the dataset into a test and train set. We determine features to employ, select a classifier, train the classifier with the train set, and test it with the test set. We also normally create a validation set to conduct a final evaluation.

We can use $m$-fold cross validation to estimate generalized classifier performance. To do so, we randomly divide the training set into $m$ even, disjoint sets. Then, we train using $(m-1)$ subsets and validate on the remaining subset. Repeat this $m$ using a different validation set every time, finally averaging the results.

**Precision** is the number of correct detections divided by the number of total detections. **Recall** is the number of correctly detections divided by the number of true events.

**K-nearest neighbors** (KNN) is one of the simplest classification strategies. It assigns a label to a point based on its neighbors, where the label becomes whatever is most common within its K nearest neighbors. We can prevent ties if $k$ is odd, and we prefer to keep $k$ small.

**Decision trees** are used to classify a pattern through a sequence of questions. Classification and regression trees (CART) is a general framework for creating decision trees. CART states that every non-binary numeric decision can be represented as a combination of binary decisions in a piecewise fashion. We should prefer decision trees that lead to the simplest tree using impurity measures. Choose a decision at node $N$ that decreases impurity the most.

We can terminate decision tree generation by growing out the full tree and then pruning negligible nodes. To assign categories to leaf nodes, we take the simplest approach and take a majority vote of class labels at the leaf node. However, there may be ties, which can be solved with random assignment, prior accounts, and considering risks.