Structure and depth are inherently ambiguous from a single view. If we have two views, we can perceive a certain amount of depth and shape by splicing the two together. However for this to work, we need to be able to locate the same object in both pictures.

Assuming parallel optical axes, we can define the expression for Z (depth) as:

$$
Z=f\frac{T}{x_l-x_r}
$$

where $p$ is the world point, $(x_l-x_r)$ is the relative depth or **disparity**, $f$ is the focal length, $T$ is the baseline, and $x_l$ and $x_r$ are the image points.

We can create a disparity map $D(x,y)$ representing the depth of the image where the maximum depth is represented with a 0 and the smallest depth is a 1.

**Epipolar constraint** is how the geometry of two views constrains where the corresponding pixels for some image is in the second image. Corresponding points must be carved out by a plane connecting the world point and optical centers. If $p$ and $p'$ exist (one in left and right), they are related by rotation and translation of the cameras.

The fundamental matrix $F$ relates $p$ and $p'$ by $p'^TFp=0$. We can solve for $F$ using point correspondences. If we have two *parallel optical axes*, we can combine them to build a depth map (inversely related to disparity maps). We use $Z=f\frac{T}{x_l-x_r}$ to find disparities. Generally, we will:

- Iterate over each pixel in the first image.
- Find corresponding epipolar scanline in the other.
- Examine pixels on the scanline and pick best match.
- Compute disparity between match and pixel and set depth to be $\frac{fT}{x-x'}$.

We can use [[10. Image Segmentation & Template Matching#Template Matching|NCC]] to find our best match. Sometimes, we only need to search left, which increases efficiency. We observe constraints with this approach:

- For any point in one image, there should be at most one matching point in the other image.
- Corresponding points should be in the same order in both views.
- We expect disparity values to change slowly, for the most part.