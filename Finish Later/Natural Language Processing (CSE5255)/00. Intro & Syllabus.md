NLP is a subfield of artificial intelligence, mediated by language. Classical AI aims to study intelligence using computational methods, and can drift into philosophy. It is hard to define intelligence.

Applied AI is about generalizable methods for achieving useful automation that approaches or exceeds human capabilities.

We treat words of as vectors or TOKENS (on average 0.7 of a word each). Think of tools such as Word2Vec, GLOVE, LSI. It's a mix of semi-supervised training of very powerful models.

Since August of last year, we have seen the following changes:

- We have serious open-source transformer models
- Efficiency gains for inference and training
- Reached peak hype cycle of GenAI
- Approached Copyright concerns, legislations, and now have insights.

This course will cover the following:

- Scientific thinking
- evaluation
- enjoyment of language
- probability and calculus
- basic linear algebra
- Python data skills
- LLM tools like PyTorch and LLM APIs
- Communication, Agile, reading and writing efficiently

# Exam Content

- Linguistics data & data structures

Ambiguity, parts of speech, context free grammars, hidden markov models

CFG allows you to talk about ambiguity.

Create HMM

- NLP datasets and tasks

Textual entailment, named entity recognition, factoid question answering and long-form question answering

We do 3 way split instead of 2 way split for training, validation, and test. The final test set is for sanity, only to be used once at the end.

Linear separability is: given a binary classifier, can you draw a line that separates the two classes? Be able to draw a diagram of both linearly separable and non-linearly separable data.

Cross validation: making the split, many different schemes:
	