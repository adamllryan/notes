## Sparseness & Dimensionalities

### Reduce this

# Dimensionality Reduction

Bins grow exponentially

### General Idea of DR

### Notation

## Principal Component Analysis: Linear dimensionality Reduction

### Karhunen-Loeve Transform

### PCA where M=1

### PCA M > 1

#### How to choose M

##### Minimize Error (Projection)

## PCA Summary

### Examples

### Reconstruction

### Applications of PCA

# Nonlinear Dimensionality Reduction

It can fail at times. We want to be able to reduce dimensionality without losing information. 

## General Idea (M < D)

Most fail, Laplacian eigenmap is kind of close for the swiss roll problem

# t-SNE

T distributed stochastic Neighbor embedding. Uses pairwise joint probabability to characterize every set of instances. It preserves the probability of something being this far from sometihng else. 

# Summary

do later lmao
